# SPDX-License-Identifier: MIT
---
- name: Set platform/version specific variables
  include_tasks: tasks/set_vars.yml

- name: Fail on unsupported architectures
  fail:
    msg: >-
      This role supports only on x86_64 architecture.
      You are running on {{ ansible_facts['architecture'] }} architecture.
      Let us know if you need the role to support it.
  when: ansible_facts['architecture'] != 'x86_64'

- name: Fail if role installs openmpi without cuda toolkit
  fail:
    msg:
      - Building OpenMPI requires multiple packages to be installed
      - You must set the following variables true to build OpenMPI with Nvidia
      - GPU support
      - "hpc_install_cuda_toolkit: true"
      - "hpc_install_nvidia_nccl: true"
  when:
    - hpc_build_openmpi_w_nvidia_gpu_support
    - not hpc_install_cuda_toolkit
    - not hpc_install_nvidia_nccl

- name: Fail if role installs NVIDIA Container Toolkit without Docker
  fail:
    msg:
      - NVIDIA Container Toolkit requires Docker to be installed
      - You must set the following variable to true
      - "hpc_install_docker: true"
  when:
    - hpc_install_nvidia_container_toolkit
    - not hpc_install_docker

- name: Fail if role installs azurehpc-health-checks without NVIDIA Container Toolkit
  fail:
    msg:
      - azurehpc-health-checks requires NVIDIA Container Toolkit to be installed
      - You must set the following variable to true
      - "hpc_install_nvidia_container_toolkit: true"
  when:
    - hpc_install_azurehpc_health_checks
    - not hpc_install_nvidia_container_toolkit

- name: Deploy GPG keys for repositories
  rpm_key:
    key: "{{ item.key }}"
    state: present
  loop:
    - "{{ __hpc_rhel_epel_repo }}"
    - "{{ __hpc_nvidia_cuda_repo }}"
    - "{{ __hpc_microsoft_prod_repo }}"

# package dkms is required from this repo
- name: Install EPEL release package
  package:
    name: "{{ __hpc_rhel_epel_repo.rpm }}"
    state: present
    use: "{{ (__hpc_server_is_ostree | d(false)) |
      ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

- name: Configure repositories
  yum_repository:
    name: "{{ item.name }}"
    description: "{{ item.description }}"
    baseurl: "{{ item.baseurl }}"
    gpgcheck: true
  loop:
    - "{{ __hpc_nvidia_cuda_repo }}"
    - "{{ __hpc_microsoft_prod_repo }}"

- name: Install base packages
  package:
    name: "{{ __hpc_base_packages }}"
    state: present
    use: "{{ (__hpc_server_is_ostree | d(false)) |
      ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
  register: __hpc_base_packages_install
  until: __hpc_base_packages_install is success

- name: Replace default RHUI Azure repository with the EUS repository
  when: hpc_enable_eus_repo
  block:
    - name: Get list of installed repositories
      command: dnf repolist
      changed_when: false
      register: __hpc_dnf_repolist

    - name: Ensure that the non-EUS RHUI Azure repository is not installed
      package:
        name: rhui-azure-rhel{{ ansible_facts['distribution_major_version'] }}
        state: absent
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

    - name: Enable the RHUI Azure EUS repository
      when: >-
        'rhui-microsoft-azure-rhel' + ansible_facts['distribution_major_version']
        + '-eus ' not in __hpc_dnf_repolist.stdout
      block:
        - name: Create a temp file for the EUS repository configuration
          tempfile:
            state: file
            prefix: rhel{{ ansible_facts['distribution_major_version'] }}-eus
            suffix: .config
          register: __hpc_euc_config

        - name: Generate the repository configuration template
          template:
            src: rhel-ver-eus.config
            dest: "{{ __hpc_euc_config.path }}"
            mode: "0644"
            owner: root
            group: root

        - name: Add EUS repository
          command: >-
            dnf --config {{ __hpc_euc_config.path }} install
            rhui-azure-rhel{{ ansible_facts['distribution_major_version'] }}-eus
            --assumeyes
          changed_when: true

        - name: Lock the RHEL minor release to the current minor release
          copy:
            content: "{{ ansible_facts['distribution_version'] }}"
            dest: /etc/dnf/vars/releasever
            mode: "0644"
            owner: root
            group: root

- name: Configure firewall to use trusted zone as default
  when: hpc_manage_firewall
  include_role:
    name: fedora.linux_system_roles.firewall
  vars:
    firewall:
      - set_default_zone: trusted
        state: enabled

- name: Configure storage
  when: hpc_manage_storage
  block:
    - name: Validate storage size formats
      assert:
        that:
          - hpc_rootlv_size is match('^[0-9]+G$')
          - hpc_usrlv_size is match('^[0-9]+G$')
          - hpc_varlv_size is match('^[0-9]+G$')
        fail_msg: |
          Storage size variables should be whole numbers in format <number>G (e.g., '15G', not '15.5G').
          Got: rootlv={{ hpc_rootlv_size }}, usrlv={{ hpc_usrlv_size }}, varlv={{ hpc_varlv_size }}

    - name: Install lvm2 to get lvs command
      package:
        name: lvm2
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

    - name: Check if rootlv exists
      stat:
        path: /dev/mapper/{{ hpc_rootvg_name }}-{{ hpc_rootlv_name }}
      register: __hpc_rootlv_stat

    - name: Check if usrlv exists
      stat:
        path: /dev/mapper/{{ hpc_rootvg_name }}-{{ hpc_usrlv_name }}
      register: __hpc_usrlv_stat

    - name: Check if varlv exists
      stat:
        path: /dev/mapper/{{ hpc_rootvg_name }}-{{ hpc_varlv_name }}
      register: __hpc_varlv_stat

    - name: Get current LV size of rootlv
      command: lvs --noheadings --units g --nosuffix -o lv_size /dev/mapper/{{ hpc_rootvg_name }}-{{ hpc_rootlv_name }}
      register: __hpc_rootlv_size_cmd
      changed_when: false
      when: __hpc_rootlv_stat.stat.exists

    - name: Get current LV size of usrlv
      command: lvs --noheadings --units g --nosuffix -o lv_size /dev/mapper/{{ hpc_rootvg_name }}-{{ hpc_usrlv_name }}
      register: __hpc_usrlv_size_cmd
      changed_when: false
      when: __hpc_usrlv_stat.stat.exists

    - name: Get current LV size of varlv
      command: lvs --noheadings --units g --nosuffix -o lv_size /dev/mapper/{{ hpc_rootvg_name }}-{{ hpc_varlv_name }}
      register: __hpc_varlv_size_cmd
      changed_when: false
      when: __hpc_varlv_stat.stat.exists

    - name: Initialize volumes list
      set_fact:
        __hpc_volumes: []

    - name: Add rootlv if exists and needs expansion
      set_fact:
        __hpc_volumes: "{{ __hpc_volumes + [volume_config] }}"
      vars:
        size_expected: "{{ hpc_rootlv_size | regex_replace('[^0-9]', '') | int }}"
        size_current: "{{ __hpc_rootlv_size_cmd.stdout | default('0') | trim | int }}"
        volume_config:
          name: "{{ hpc_rootlv_name }}"
          size: "{{ hpc_rootlv_size }}"
          mount_point: "{{ hpc_rootlv_mount }}"
      when:
        - __hpc_rootlv_stat.stat.exists
        - (size_expected | int) > (size_current | int)

    - name: Add usrlv if exists and needs expansion
      set_fact:
        __hpc_volumes: "{{ __hpc_volumes + [volume_config] }}"
      vars:
        size_expected: "{{ hpc_usrlv_size | regex_replace('[^0-9]', '') | int }}"
        size_current: "{{ __hpc_usrlv_size_cmd.stdout | default('0') | trim | int }}"
        volume_config:
          name: "{{ hpc_usrlv_name }}"
          size: "{{ hpc_usrlv_size }}"
          mount_point: "{{ hpc_usrlv_mount }}"
      when:
        - __hpc_usrlv_stat.stat.exists
        - (size_expected | int) > (size_current | int)

    - name: Add varlv if exists and needs expansion
      set_fact:
        __hpc_volumes: "{{ __hpc_volumes + [volume_config] }}"
      vars:
        size_expected: "{{ hpc_varlv_size | regex_replace('[^0-9]', '') | int }}"
        size_current: "{{ __hpc_varlv_size_cmd.stdout | default('0') | trim | int }}"
        volume_config:
          name: "{{ hpc_varlv_name }}"
          size: "{{ hpc_varlv_size }}"
          mount_point: "{{ hpc_varlv_mount }}"
      when:
        - __hpc_varlv_stat.stat.exists
        - (size_expected | int) > (size_current | int)

    - name: Configure storage
      include_role:
        name: fedora.linux_system_roles.storage
      vars:
        storage_pools:
          - name: "{{ hpc_rootvg_name }}"
            grow_to_fill: true
            volumes: "{{ __hpc_volumes }}"
      when: __hpc_volumes | length > 0

- name: Force install kernel version
  package:
    name: kernel-{{ __hpc_force_kernel_version }}
    state: present
    allow_downgrade: true
  when: __hpc_force_kernel_version is not none
  notify: Reboot system

- name: Update kernel
  when:
    - hpc_update_kernel
    - __hpc_force_kernel_version is none
  package:
    name: kernel
    state: latest  # noqa package-latest
  notify: Reboot system

- name: Get package facts
  package_facts:
  no_log: true

# This is required for dkms to build NVidia drivers for all kernels
- name: Install kernel-devel and kernel-headers packages for all kernels
  vars:
    kernel_version: "{{ item.version }}-{{ item.release }}"
  package:
    name:
      - kernel-devel-{{ kernel_version }}
      - kernel-headers-{{ kernel_version }}
    state: present
  notify: Reboot system
  loop: "{{ ansible_facts.packages.kernel }}"

- name: Ensure that dnf-command(versionlock) is installed
  package:
    name: dnf-command(versionlock)
    state: present

- name: Check if kernel versionlock entries exist
  stat:
    path: "{{ __hpc_versionlock_path }}"
  register: __hpc_versionlock_stat

# once nvidia drivers are built for a specific kernel version, we need to
# prevent installation of all kernel packages of a different version
# MS unsuccessfully tries to do this in azhpc-images build scripts:
# https://github.com/Azure/azhpc-images/blob/a3f92d283af6c9d11bf08eb0f8763ab67f7c8713/partners/rhel/common/install_utils.sh#L61
- name: Prevent installation of all kernel packages of a different version
  command: dnf versionlock add {{ item }}
  register: __hpc_versionlock_check
  changed_when: >-
    'Package already locked in equivalent form'
    not in __hpc_versionlock_check.stdout
  loop: "{{ __hpc_kernel_versionlock_rpms }}"

- name: Update all packages to bring system to the latest state
  when: hpc_update_all_packages
  package:
    name: "*"
    state: latest  # noqa package-latest

- name: Install Azure-specific platform packages
  when: ansible_facts["system_vendor"] == "Microsoft Corporation"
  block:
    - name: Install Azure platform packages
      environment:
        AZNFS_NONINTERACTIVE_INSTALL: "1"
      package:
        name: "{{ __hpc_azure_packages }}"
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_azure_packages_install
      until: __hpc_azure_packages_install is success
      
- name: Create Azure HPC resource directories
  file:
    path: "{{ item }}"
    state: directory
    owner: root
    group: root
    mode: '0755'
  loop:
    - "{{ __hpc_azure_resource_dir }}"
    - "{{ __hpc_azure_resource_dir }}/bin"
    - "{{ __hpc_azure_tools_dir }}"
    - "{{ __hpc_azure_tests_dir }}"
    - "{{ __hpc_azure_runtime_dir }}"

- name: Install NVidia driver
  # Note that currently the role supports only Microsoft Azure
  # When we add more cloud providers, we need to update this condition
  when: ansible_facts["system_vendor"] == "Microsoft Corporation"
  block:
    - name: Get list of enabled nvidia-driver modules
      command: dnf module list --enabled nvidia-driver
      register: __hpc_nvidia_enabled
      failed_when: false
      changed_when: false
    - name: Reset nvidia-driver module and remove old/different nvidia driver packages
      when:
        - __hpc_nvidia_enabled.stdout is search('nvidia-driver')
        - __hpc_nvidia_driver_stream not in __hpc_nvidia_enabled.stdout
      block:
        - name: Reset nvidia-driver module if different version is enabled
          command: dnf module reset nvidia-driver --assumeyes
          changed_when: true

        - name: Remove NVIDIA driver packages if old/different version is present
          package:
            name: "{{ __hpc_nvidia_driver_packages + __hpc_nvidia_fabric_manager_packages }}"
            state: absent
          register: __hpc_nvidia_old_removed

    - name: Enable NVIDIA driver module
      when: __hpc_nvidia_driver_stream not in __hpc_nvidia_enabled.stdout
      command: dnf module enable {{ __hpc_nvidia_driver_module }} --assumeyes
      notify: Reboot system
      changed_when: true

    - name: Install dkms
      package:
        name: "{{ __hpc_dkms_packages }}"
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_dkms_packages_install
      until: __hpc_dkms_packages_install is success

    - name: Install NVIDIA driver
      package:
        name: "{{ __hpc_nvidia_driver_packages }}"
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_nvidia_driver_packages_install
      until: __hpc_nvidia_driver_packages_install is success

    # This makes the role not idempotent.
    # We need to find a condition in which starting and enabling is not enough.
    - name: Restart dkms service to make it build nvidia drivers for all kernels
      service:
        name: dkms.service
        enabled: true
        state: restarted

- name: Install CUDA driver and enable nvidia-persistenced.service
  when:
    - ansible_facts["system_vendor"] == "Microsoft Corporation"
    - hpc_install_cuda_driver
  block:
    - name: Install CUDA driver
      package:
        name: "{{ __hpc_cuda_driver_packages }}"
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_cuda_driver_packages_install
      until: __hpc_cuda_driver_packages_install is success

    - name: Enable nvidia-persistenced.service
      service:
        name: nvidia-persistenced.service
        enabled: true

- name: Install CUDA Toolkit and lock version of its packages
  when:
    - hpc_install_cuda_toolkit
    - ansible_facts["system_vendor"] == "Microsoft Corporation"
  block:
    - name: Install CUDA Toolkit
      package:
        name: "{{ __hpc_cuda_toolkit_packages }}"
        state: present
        allow_downgrade: true
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_cuda_toolkit_packages_install
      until: __hpc_cuda_toolkit_packages_install is success

    - name: Prevent update of CUDA Toolkit packages
      command: dnf versionlock add {{ item }}
      register: __hpc_versionlock_check
      changed_when: >-
        'Package already locked in equivalent form'
        not in __hpc_versionlock_check.stdout
      loop: "{{ __hpc_kernel_versionlock_rpms }}"

- name: Install NVIDIA NCCL and lock version of its packages
  when: hpc_install_hpc_nvidia_nccl
  block:
    - name: Install NVIDIA NCCL
      package:
        name: "{{ __hpc_nvidia_nccl_packages }}"
        state: present
        allow_downgrade: true
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_nvidia_nccl_packages_install
      until: __hpc_nvidia_nccl_packages_install is success

    - name: Prevent update of NVIDIA NCCL packages
      command: dnf versionlock add {{ item }}
      register: __hpc_versionlock_check
      changed_when: >-
        'Package already locked in equivalent form'
        not in __hpc_versionlock_check.stdout
      loop: "{{ __hpc_nvidia_nccl_packages }}"

- name: Install NVIDIA Fabric Manager and enable service
  when: hpc_install_nvidia_fabric_manager
  block:
    - name: Install NVIDIA Fabric Manager
      package:
        name: "{{ __hpc_nvidia_fabric_manager_packages }}"
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_nvidia_fabric_manager_packages_install
      until: __hpc_nvidia_fabric_manager_packages_install is success

    - name: Ensure that Fabric Manager service is enabled
      service:
        name: nvidia-fabricmanager
        enabled: true

- name: Install RDMA packages
  when: hpc_install_rdma
  block:
    - name: Install RDMA packages
      package:
        name: "{{ __hpc_rdma_packages }}"
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_rdma_packages_install
      until: __hpc_rdma_packages_install is success

    # azhpc-images build scripts do the same thing here:
    # https://github.com/Azure/azhpc-images/blob/a3f92d283af6c9d11bf08eb0f8763ab67f7c8713/common/install_waagent.sh#L53
    - name: Enable RDMA in waagent configuration
      when: ansible_facts['system_vendor'] == 'Microsoft Corporation'
      lineinfile:
        path: /etc/waagent.conf
        line: "OS.EnableRDMA=y"
        create: true
        backup: true
        mode: "0644"
      notify: Restart waagent

    - name: Configure Azure persistent RDMA naming (systemd + udev)
      when:
        - hpc_enable_azure_persistent_rdma_naming
        - ansible_facts['system_vendor'] == 'Microsoft Corporation'
      block:
        - name: Install Azure persistent RDMA naming script
          template:
            src: rdma/azure_persistent_rdma_naming.sh.j2
            dest: /usr/sbin/azure_persistent_rdma_naming.sh
            owner: root
            group: root
            mode: "0755"

        - name: Install systemd service for Azure persistent RDMA naming
          template:
            src: rdma/azure_persistent_rdma_naming.service.j2
            dest: /etc/systemd/system/azure_persistent_rdma_naming.service
            owner: root
            group: root
            mode: "0644"
          register: __hpc_azure_persistent_rdma_naming_unit

        - name: Install udev rule to trigger persistent naming on IB device changes
          template:
            src: rdma/99-azure-persistent-rdma-naming.rules.j2
            dest: /etc/udev/rules.d/99-azure-persistent-rdma-naming.rules
            owner: root
            group: root
            mode: "0644"
          notify:
            - Reload udev
            - Trigger udev for infiniband

        - name: Enable and start Azure persistent RDMA naming service
          systemd:
            name: azure_persistent_rdma_naming.service
            enabled: true
            state: started
            daemon_reload: "{{ __hpc_azure_persistent_rdma_naming_unit.changed | d(false) }}"

        - name: Install Azure persistent RDMA naming monitor script
          template:
            src: rdma/azure_persistent_rdma_naming_monitor.sh.j2
            dest: /usr/sbin/azure_persistent_rdma_naming_monitor.sh
            owner: root
            group: root
            mode: "0755"

        - name: Install systemd service for Azure persistent RDMA naming monitor
          template:
            src: rdma/azure_persistent_rdma_naming_monitor.service.j2
            dest: /etc/systemd/system/azure_persistent_rdma_naming_monitor.service
            owner: root
            group: root
            mode: "0644"
          register: __hpc_azure_persistent_rdma_naming_monitor_unit

        - name: Enable and start Azure persistent RDMA naming monitor service
          systemd:
            name: azure_persistent_rdma_naming_monitor.service
            enabled: true
            state: started
            daemon_reload: "{{ __hpc_azure_persistent_rdma_naming_monitor_unit.changed | d(false) }}"

    - name: Install RDMA validation script
      copy:
        src: rdma/test-rdma.sh
        dest: "{{ __hpc_azure_tests_dir }}/test-rdma.sh"
        owner: root
        group: root
        mode: "0755"

- name: Install common OpenMPI packages
  when: hpc_install_system_openmpi or hpc_build_openmpi_w_nvidia_gpu_support
  package:
    name: "{{ __hpc_openmpi_common_packages }}"
    state: present
    use: "{{ (__hpc_server_is_ostree | d(false)) |
      ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
  register: __hpc_openmpi_common_packages_install
  until: __hpc_openmpi_common_packages_install is success

- name: Install system OpenMPI
  when: hpc_install_system_openmpi
  package:
    name: "{{ __hpc_system_openmpi_packages }}"
    state: present
    use: "{{ (__hpc_server_is_ostree | d(false)) |
      ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
  register: __hpc_system_openmpi_packages_install
  until: __hpc_system_openmpi_packages_install is success

- name: Download, build, and configure OpenMPI and all required packages
  when: hpc_build_openmpi_w_nvidia_gpu_support
  block:
    - name: Install build dependencies
      package:
        name: >-
          {{ __hpc_pmix_build_dependencies
          + __hpc_gdrcopy_build_dependencies
          + __hpc_openmpi_build_dependencies }}
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

    # Must keep the original tarball name because it's hardcoded in hpcx
    - name: Set __hpc_hpcx_path fact
      vars:
        hpcx_tarball_basename: "{{ __hpc_hpcx_info.url | basename
          | regex_replace('\\.[^.]*$', '') }}"
      set_fact:
        __hpc_hpcx_path: "{{ __hpc_install_prefix }}/{{ hpcx_tarball_basename }}"

    - name: Set facts for building HPC-X and OpenMPI
      set_fact:
        __hpc_hpcx_rebuild_path: "{{ __hpc_hpcx_path }}/hpcx-rebuild"
        __hpc_hcoll_path: "{{ __hpc_hpcx_path }}/hcoll"
        __hpc_ucx_path: "{{ __hpc_hpcx_path }}/ucx/hpcx-rebuild"
        __hpc_ucc_path: "{{ __hpc_hpcx_path }}/ucc"
        __hpc_pmix_path: "{{ __hpc_install_prefix }}/{{ __hpc_pmix_info.name }}/{{ __hpc_pmix_info.version }}"
        __hpc_openmpi_path: "{{ __hpc_install_prefix }}/{{ __hpc_openmpi_info.name }}-{{ __hpc_openmpi_info.version }}"
        __hpc_cuda_path: /usr/local/cuda

    - name: Get stat of pmix path
      stat:
        path: "{{ __hpc_pmix_path }}/bin/pmixcc"
      register: __hpc_pmix_path_stat

    - name: Download and build PMIx
      when: not __hpc_pmix_path_stat.stat.exists
      block:
        - name: Download PMIx
          include_tasks: tasks/download_extract_package.yml
          vars:
            __hpc_pkg_info: "{{ __hpc_pmix_info }}"

        - name: Build PMIx
          command:
            cmd: "{{ item }}"
            chdir: "{{ __hpc_pkg_extracted.path }}"
          changed_when: true
          loop:
            - >-
              ./configure --prefix={{ __hpc_pmix_path }}
              --enable-pmix-binaries
              --disable-dependency-tracking
            - make -j {{ ansible_facts["processor_nproc"] }}
            - make install

    - name: Ensure PMIx modulefile directory exists
      file:
        path: "{{ __hpc_module_dir }}/pmix"
        state: directory
        owner: root
        group: root
        mode: '0755'

    # The openmpi-{{ __hpc_openmpi_info.version }} env module depends on this
    - name: Install PMIx modulefile
      template:
        src: pmix-ver.lua
        dest: "{{ __hpc_module_dir }}/pmix/pmix-{{ __hpc_pmix_info.version }}.lua"
        owner: root
        group: root
        mode: '0755'

    - name: Install GDRCopy packages
      vars:
        gdrcopy_version: "{{ __hpc_gdrcopy_info.version | split('-') | first }}"
      when: >-
        (ansible_facts.packages.gdrcopy is not defined
        or ansible_facts.packages.gdrcopy[0].version != gdrcopy_version )
        or (ansible_facts.packages["gdrcopy-kmod"] is not defined
        or ansible_facts.packages["gdrcopy-kmod"][0].version != gdrcopy_version)
        or (ansible_facts.packages["gdrcopy-devel"] is not defined
        or ansible_facts.packages["gdrcopy-devel"][0].version != gdrcopy_version)
      block:
        - name: Download GDRCopy
          include_tasks: tasks/download_extract_package.yml
          vars:
            __hpc_pkg_info: "{{ __hpc_gdrcopy_info }}"

        - name: Build GDRCopy RPM packages
          environment:
            CUDA: "{{ __hpc_cuda_path }}"
          command:
            cmd: packages/build-rpm-packages.sh
            chdir: "{{ __hpc_pkg_extracted.path }}"
          changed_when: true

        - name: Install GDRCopy packages from built RPMs
          package:
            name:
              - "{{ __hpc_pkg_extracted.path }}/gdrcopy-kmod-{{ __hpc_gdrcopy_info.version }}dkms.{{ __hpc_gdrcopy_info.distribution }}.noarch.rpm"
              - "{{ __hpc_pkg_extracted.path }}/gdrcopy-{{ __hpc_gdrcopy_info.version }}.{{ __hpc_gdrcopy_info.distribution }}.x86_64.rpm"
              - "{{ __hpc_pkg_extracted.path }}/gdrcopy-devel-{{ __hpc_gdrcopy_info.version }}.{{ __hpc_gdrcopy_info.distribution }}.noarch.rpm"
            disable_gpg_check: true
            state: present
            use: "{{ (__hpc_server_is_ostree | d(false)) |
              ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

        - name: Remove extracted tarball
          file:
            path: "{{ __hpc_pkg_extracted.path }}"
            state: absent
          changed_when: false

    - name: Get stat of hpcx-rebuild path
      stat:
        path: "{{ __hpc_hpcx_rebuild_path }}"
      register: __hpc_hpcx_rebuild_path_stat

    - name: Download and build HPC-X
      when: not __hpc_hpcx_rebuild_path_stat.stat.exists
      block:
        - name: Download HPC-X
          include_tasks: tasks/download_extract_package.yml
          vars:
            __hpc_pkg_info: "{{ __hpc_hpcx_info }}"

        # Packages are installed in /opt, not "/build-result", so pkgconfig
        # files used by openmpi's cmake configure scripts need to be updated.
        - name: Ensure that pkgconfig files use hpcx_home={{ __hpc_hpcx_path }}
          replace:
            path: "{{ item }}"
            regexp: "/build-result/"
            replace: "{{ __hpc_install_prefix }}/"
            backup: true
          loop:
            - "{{ __hpc_pkg_extracted.path }}/hcoll/lib/pkgconfig/hcoll.pc"
            - "{{ __hpc_pkg_extracted.path }}/ucc/lib/pkgconfig/ucc.pc"
            - "{{ __hpc_pkg_extracted.path }}/ucx/lib/pkgconfig/ucx.pc"

        # the shipped pkgconfig for hcoll only includes -lhcoll. However, this
        # library depends on -locoms, so we have to add that to prevent openmpi
        # from failing to link against libhcoll.
        - name: Update hcoll pkgconfig file to add -locoms parameter
          replace:
            path: "{{ __hpc_pkg_extracted.path }}/hcoll/lib/pkgconfig/hcoll.pc"
            regexp: "-lhcoll"
            replace: "-lhcoll -locoms"
            backup: true

        - name: Copy HPC-X files to {{ __hpc_hpcx_path }}
          copy:
            src: "{{ __hpc_pkg_extracted.path }}/"
            remote_src: true
            dest: "{{ __hpc_hpcx_path }}"
            mode: "0755"
            owner: root
            group: root

        - name: Rebuild HPC-X with PMIx
          environment:
            CUDA_HOME: "{{ __hpc_cuda_path }}"
          command: >-
            {{ __hpc_hpcx_path }}/utils/hpcx_rebuild.sh
            --with-hcoll
            --cuda
            --rebuild-ucx
            --ompi-extra-config
            '--with-pmix={{ __hpc_pmix_path }}
            --enable-orterun-prefix-by-default'
          changed_when: true

        - name: Copy ompi/tests to hpcx-rebuild in {{ __hpc_hpcx_path }}
          copy:
            src: "{{ __hpc_hpcx_path }}/ompi/tests/"
            remote_src: true
            dest: "{{ __hpc_hpcx_rebuild_path }}"
            mode: "0755"
            owner: root
            group: root

        - name: Remove extracted tarball
          file:
            path: "{{ __hpc_pkg_extracted.path }}"
            state: absent
          changed_when: false

    - name: Ensure MPI modulefile directory exists
      file:
        path: "{{ __hpc_module_dir }}/mpi"
        state: directory
        owner: root
        group: root
        mode: '0755'

    - name: Install NVidia HPCX OpemMPI modulefile
      template:
        src: hpcx-ver.lua
        dest: >-
          {{ __hpc_module_dir }}/mpi/hpcx-{{ __hpc_hpcx_info.version }}.lua
        owner: root
        group: root
        mode: '0755'

    - name: Install NVidia HPCX OpemMPI with PMIx {{ __hpc_pmix_info.version }}
      template:
        src: hpcx-ver-pmix-ver.lua
        dest: >-
          {{ __hpc_module_dir }}/mpi/hpcx-{{
          __hpc_hpcx_info.version }}-pmix-{{ __hpc_pmix_info.version }}.lua
        owner: root
        group: root
        mode: '0755'

    - name: Get stat of openmpi path
      stat:
        path: "{{ __hpc_openmpi_path }}"
      register: __hpc_openmpi_path_stat

    - name: Download and build OpenMPI
      when: not __hpc_openmpi_path_stat.stat.exists
      block:
        - name: Download {{ __hpc_openmpi_info.name }}
          include_tasks: tasks/download_extract_package.yml
          vars:
            __hpc_pkg_info: "{{ __hpc_openmpi_info }}"

        - name: Build {{ __hpc_openmpi_info.name }}
          command:
            cmd: "{{ item }}"
            chdir: "{{ __hpc_pkg_extracted.path }}"
          changed_when: true
          loop:
            - >-
              ./configure --prefix={{ __hpc_openmpi_path }}
              --with-ucx={{ __hpc_ucx_path }}
              --with-ucc={{ __hpc_ucc_path }}
              --with-hcoll={{ __hpc_hcoll_path }}
              --with-pmix={{ __hpc_pmix_path }}
              --enable-prte-prefix-by-default
              --with-platform=contrib/platform/mellanox/optimized
              --with-cuda={{ __hpc_cuda_path }}
            - make -j {{ ansible_facts["processor_nproc"] }}
            - make install

        - name: Remove extracted tarball
          file:
            path: "{{ __hpc_pkg_extracted.path }}"
            state: absent
          changed_when: false

    - name: Install OpenMPI modulefile
      template:
        src: openmpi-ver-cuda12-gpu.lua
        dest: "{{ __hpc_module_dir }}/mpi/openmpi-{{ __hpc_openmpi_info.version }}-cuda12-gpu.lua"
        owner: root
        group: root
        mode: '0755'

- name: Install Docker via moby-engine and moby-cli
  when: hpc_install_docker
  block:
    - name: Install Docker packages
      package:
        name: "{{ __hpc_docker_packages }}"
        state: present
        allow_downgrade: true
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_docker_packages_install
      until: __hpc_docker_packages_install is success

    - name: Enable and start Docker service
      service:
        name: docker
        enabled: true
        state: started

    - name: Prevent update of Docker packages
      command: dnf versionlock add {{ item }}
      register: __hpc_versionlock_check
      changed_when: >-
        'Package already locked in equivalent form'
        not in __hpc_versionlock_check.stdout
      loop: "{{ __hpc_docker_packages }}"

- name: Install and configure NVIDIA Container Toolkit
  when: hpc_install_nvidia_container_toolkit
  block:
    - name: Install NVIDIA Container Toolkit
      package:
        name: "{{ __hpc_nvidia_container_toolkit_packages }}"
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_nvidia_container_toolkit_install
      until: __hpc_nvidia_container_toolkit_install is success

    - name: Check if NVIDIA runtime is configured in Docker daemon
      find:
        paths: /etc/docker
        patterns: daemon.json
        contains: nvidia
        read_whole_file: true
      register: docker_nvidia_runtime_check

    - name: Configure NVIDIA Container Toolkit for Docker runtime
      command: nvidia-ctk runtime configure --runtime=docker
      when: docker_nvidia_runtime_check.matched == 0
      changed_when: true
      notify: Restart docker service

    - name: Ensure containerd config directory exists
      file:
        path: /etc/containerd
        state: directory
        owner: root
        group: root
        mode: '0755'

    - name: Generate default containerd config
      shell:
        cmd: containerd config default > /etc/containerd/config.toml
        creates: /etc/containerd/config.toml

    - name: Enable SystemdCgroup in containerd config
      lineinfile:
        path: /etc/containerd/config.toml
        regexp: '^\s*SystemdCgroup\s*='
        line: '            SystemdCgroup = true'
        backup: true

    - name: Check if NVIDIA Container Toolkit drop-in file exists
      stat:
        path: /etc/containerd/conf.d/99-nvidia.toml
      register: nvidia_containerd_dropin

    - name: Check if containerd config has drop-in imports
      find:
        paths: /etc/containerd
        patterns: config.toml
        contains: 'imports = \["/etc/containerd/conf\.d/\*\.toml"\]'
      register: containerd_imports_check

    - name: Configure NVIDIA Container Toolkit for containerd runtime
      command: nvidia-ctk runtime configure --runtime=containerd --set-as-default
      when: not nvidia_containerd_dropin.stat.exists or containerd_imports_check.matched == 0
      changed_when: true
      notify: Restart containerd service

- name: Tune system for HPC
  when: hpc_tuning
  block:
    - name: Remove user memory limits to ensure applications aren't restricted
      template:
        src: 90-hpc-limits.conf
        dest: /etc/security/limits.d/
        owner: root
        group: root
        mode: "0644"

    - name: Add sysctl tuning configuration for HPC
      template:
        src: 90-hpc-sysctl.conf
        dest: /etc/sysctl.d/
        owner: root
        group: root
        mode: '0644'
      notify: Reload sysctl

    - name: Load sunrpc kernel module
      lineinfile:
        path: /etc/modules-load.d/sunrpc.conf
        line: sunrpc
        create: true
        owner: root
        group: root
        mode: '0644'
      notify: Restart systemd-modules-load

    - name: Check if sunrpc module is loaded
      command: lsmod
      register: __hpc_loaded_modules
      changed_when: false

    - name: Load sunrpc module if not loaded
      when: "'sunrpc' not in __hpc_loaded_modules.stdout"
      command: modprobe sunrpc
      changed_when: true

    - name: Copy NFS readahead udev rules for Azure infrastructure
      template:
        src: 90-nfs-readahead.rules
        dest: /etc/udev/rules.d/
        owner: root
        group: root
        mode: '0644'
      notify: Reload udev

- name: Install SKU Customisation scripts and services
  when: hpc_sku_customisation
  block:
    - name: Check if already installed
      stat:
        path: "{{ __hpc_azure_resource_dir }}/topology"
      register: __hpc_sku_topology_stat

    - name: Install files
      when: not __hpc_sku_topology_stat.stat.exists
      block:
        - name: Install Topology Definitions
          copy:
            src: sku/topology/
            dest: "{{ __hpc_azure_resource_dir }}/topology"
            owner: root
            group: root
            mode: '0755'

        - name: Install Graph Files
          copy:
            src: sku/customisations/
            dest: "{{ __hpc_azure_resource_dir }}/customisations"
            owner: root
            group: root
            mode: '0755'

        - name: Install setup script
          template:
            src: sku/setup_sku_customisations.sh
            dest: "{{ __hpc_azure_resource_dir }}/bin/setup_sku_customisations.sh"
            owner: root
            group: root
            mode: '0755'

        - name: Install removal script
          template:
            src: sku/remove_sku_customisations.sh
            dest: "{{ __hpc_azure_resource_dir }}/bin/remove_sku_customisations.sh"
            owner: root
            group: root
            mode: '0755'

        - name: Install systemd service file
          template:
            src: sku/sku_customisation.service
            dest: /etc/systemd/system/
            owner: root
            group: root
            mode: '0755'

        - name: Enable systemd service file
          service:
            name: sku_customisation.service
            enabled: true

        - name: Install tests
          template:
            src: sku/test-sku-setup.sh
            dest: "{{ __hpc_azure_tests_dir }}/"
            owner: root
            group: root
            mode: '0755'

- name: Remove build dependencies
  vars:
    __hpc_dependencies: >-
      {{ __hpc_gdrcopy_build_dependencies
      + __hpc_pmix_build_dependencies
      + __hpc_openmpi_build_dependencies }}
  package:
    name: "{{ __hpc_dependencies }}"
    state: absent
    use: "{{ (__hpc_server_is_ostree | d(false)) |
      ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

- name: Install Moneo monitoring tool
  when: hpc_install_moneo
  block:
    - name: Check if Moneo is already installed
      stat:
        path: "{{ __hpc_azure_moneo_dir }}/moneo.py"
      register: __hpc_moneo_installed

    - name: Download and install Moneo
      when: not __hpc_moneo_installed.stat.exists
      block:
        - name: Ensure Moneo install directory exists
          file:
            path: "{{ __hpc_azure_tools_dir }}"
            state: directory
            mode: '0755'
            owner: root
            group: root

        - name: Download Moneo
          include_tasks: download_extract_package.yml
          vars:
            __hpc_pkg_info: "{{ __hpc_moneo_info }}"

        - name: Copy Moneo files to install directory
          copy:
            src: "{{ __hpc_pkg_extracted.path }}/"
            remote_src: true
            dest: "{{ __hpc_azure_moneo_dir }}"
            mode: '0755'
            owner: root
            group: root

        - name: Patch hardcoded paths in Moneo scripts
          replace:
            path: "{{ __hpc_azure_moneo_dir }}/linux_service/configure_service.sh"
            regexp: '/opt/azurehpc/tools/Moneo'
            replace: "{{ __hpc_azure_moneo_dir }}"

        - name: Configure Moneo service
          command: "{{ __hpc_azure_moneo_dir }}/linux_service/configure_service.sh"
          register: __hpc_moneo_configure
          changed_when: "'already configured' not in __hpc_moneo_configure.stdout"

    - name: Ensure container registries directory exists
      file:
        path: /etc/containers/registries.conf.d
        state: directory
        mode: '0755'
        owner: root
        group: root

    - name: Configure container registries for short-name resolution
      copy:
        dest: /etc/containers/registries.conf.d/99-unqualified-search.conf
        mode: '0644'
        owner: root
        group: root
        content: |
          [aliases]
          "prometheus" = "docker.io/prom/prometheus"

    - name: Ensure containers config directory exists
      file:
        path: /etc/containers/containers.conf.d
        state: directory
        mode: '0755'
        owner: root
        group: root

    - name: Ensure tools for managing SELinux file contexts are installed
      package:
        name:
          - policycoreutils-python-utils
        state: present
      when:
        - ansible_selinux is defined
        - ansible_selinux.status | d('disabled') == 'enabled'

    - name: Allow containers to read Prometheus config (SELinux fcontext)
      shell: |
        set -euo pipefail
        if semanage fcontext -l | grep -Eq '^/etc/prometheus\(/\.\\*\)\?\s+all files\s+.*container_file_t'; then
          echo "ok"
        else
          semanage fcontext -a -t container_file_t '/etc/prometheus(/.*)?' 2>/dev/null \
            || semanage fcontext -m -t container_file_t '/etc/prometheus(/.*)?'
          echo "changed"
        fi
      register: __hpc_prometheus_fcontext
      changed_when: "'changed' in __hpc_prometheus_fcontext.stdout"
      when:
        - ansible_selinux is defined
        - ansible_selinux.status | d('disabled') == 'enabled'

    - name: Apply SELinux context labels for Prometheus config directory
      command: restorecon -Rv /etc/prometheus
      changed_when: false
      failed_when: false
      when:
        - ansible_selinux is defined
        - ansible_selinux.status | d('disabled') == 'enabled'

    - name: Remove extracted temp directory
      when: not __hpc_moneo_installed.stat.exists
      file:
        path: "{{ __hpc_pkg_extracted.path }}"
        state: absent
      changed_when: false

    - name: Add Moneo alias to /etc/bashrc
      lineinfile:
        path: /etc/bashrc
        line: "alias moneo='python3 {{ __hpc_azure_moneo_dir }}/moneo.py'"
        state: present
        mode: '0644'

    - name: Install tests
      template:
        src: test-moneo.sh.j2
        dest: "{{ __hpc_azure_tests_dir }}/test-moneo.sh"
        owner: root
        group: root
        mode: '0755'

- name: Install KVP client
  block:
    - name: Check if KVP client is already installed
      stat:
        path: "{{ __hpc_azure_tools_dir }}/kvp_client"
      register: __hpc_kvp_installed

    - name: Download and compile KVP client
      when: not __hpc_kvp_installed.stat.exists
      block:
        - name: Create temporary directory for KVP client
          tempfile:
            state: directory
            prefix: kvp_client_build_
          register: __hpc_kvp_temp_dir

        - name: Download KVP client source code to temp directory
          get_url:
            url: "{{ __hpc_kvp_client_info.url }}"
            dest: "{{ __hpc_kvp_temp_dir.path }}/{{ __hpc_kvp_client_info.name }}"
            checksum: "sha256:{{ __hpc_kvp_client_info.sha256 }}"
            mode: '0644'

        - name: Compile KVP client in temp directory
          command:
            cmd: gcc {{ __hpc_kvp_temp_dir.path }}/kvp_client.c -o {{ __hpc_kvp_temp_dir.path }}/kvp_client
            creates: "{{ __hpc_kvp_temp_dir.path }}/kvp_client"

        - name: Copy compiled KVP client to {{ __hpc_azure_tools_dir }}
          copy:
            src: "{{ __hpc_kvp_temp_dir.path }}/kvp_client"
            dest: "{{ __hpc_azure_tools_dir }}/kvp_client"
            remote_src: true
            owner: root
            group: root
            mode: '0755'

        - name: Clean up temporary KVP client build directory
          file:
            path: "{{ __hpc_kvp_temp_dir.path }}"
            state: absent

- name: Install azurehpc-health-checks
  when: hpc_install_azurehpc_health_checks
  block:
    - name: Check if azurehpc-health-checks aznhc-nv Docker image exists
      command: docker images -q mcr.microsoft.com/aznhc/aznhc-nv:latest
      register: __hpc_aznhc_docker_image
      changed_when: false
      failed_when: false

    - name: Check if azurehpc-health-checks directory exists
      stat:
        path: "{{ __hpc_azure_tests_dir }}/azurehpc-health-checks"
      register: __hpc_aznhc_dir

    - name: Determine if installation is complete
      set_fact:
        __hpc_aznhc_installed: "{{ __hpc_aznhc_docker_image.stdout != '' and __hpc_aznhc_dir.stat.exists }}"

    - name: Check for existing installation
      debug:
        msg: "azurehpc-health-checks found. Checking for updates and ensuring latest version..."
      when: __hpc_aznhc_installed | bool

    - name: Determine if files need to be extracted
      set_fact:
        __hpc_aznhc_need_extract: "{{ not __hpc_aznhc_dir.stat.exists }}"

    # Only check /var free space if Docker image doesn't exist yet (first-time installation)
    - name: Check /var free space for first-time aznhc-nv Docker image download
      when: __hpc_aznhc_docker_image.stdout == ''
      vars:
        __hpc_var_free_gb: "{{ __hpc_var_free_cmd.stdout_lines[1] | regex_replace('G', '') | trim | int }}"
        __hpc_var_has_space: "{{ (__hpc_var_free_gb | int) >= 20 }}"
      block:
        - name: Get /var free space
          command: df -BG /var --output=avail
          register: __hpc_var_free_cmd
          changed_when: false

        - name: Warn if /var free space is too small for aznhc-nv Docker image pull
          fail:
            msg: >-
              WARNING: /var free space ({{ __hpc_var_free_gb }}G) is less than the required 20G.
              Skipping azurehpc-health-checks installation because aznhc-nv Docker image cannot be pulled.
              To install health checks, increase /var space by:
              (1) Expanding the root disk in Azure portal,
              (2) Adding the new space as a PV to the volume group (e.g., pvresize /dev/sda2),
              (3) Expanding the /var logical volume (e.g., lvextend -L +20G /dev/rootvg/varlv && xfs_growfs /var).
              See https://learn.microsoft.com/en-us/azure/virtual-machines/linux/expand-disks?tabs=rhellvm for details.
              Alternatively, configure hpc_varlv_size before running this role.
          when: not __hpc_var_has_space

        - name: Set skip flag if /var has insufficient free space
          set_fact:
            __hpc_skip_healthcheck: "{{ not __hpc_var_has_space }}"

    # If image already exists, skip the space check
    - name: Skip space check when image already exists
      set_fact:
        __hpc_skip_healthcheck: false
      when: __hpc_aznhc_docker_image.stdout != ''

    - name: Configure and install azurehpc-health-checks files
      when: __hpc_aznhc_need_extract | bool and not __hpc_skip_healthcheck | bool
      block:
        - name: Download and extract azurehpc-health-checks
          include_tasks: download_extract_package.yml
          vars:
            __hpc_pkg_info: "{{ __hpc_aznhc_info }}"

        - name: Replace hardcoded /opt/azurehpc/tools paths in temporary getPhysHostName.py
          replace:
            path: "{{ __hpc_pkg_extracted.path }}/getPhysHostName.py"
            regexp: '/opt/azurehpc/tools'
            replace: '{{ __hpc_azure_tools_dir }}'

        - name: Replace hardcoded /opt/azurehpc/tools paths in temporary export_nhc_result_to_kusto.py
          replace:
            path: "{{ __hpc_pkg_extracted.path }}/distributed_nhc/export_nhc_result_to_kusto.py"
            regexp: '/opt/azurehpc/tools'
            replace: '{{ __hpc_azure_tools_dir }}'

        - name: Create azurehpc-health-checks directory
          file:
            path: "{{ __hpc_azure_tests_dir }}/azurehpc-health-checks"
            state: directory
            mode: "0755"

        - name: Copy extracted files for azurehpc-health-checks
          copy:
            src: "{{ __hpc_pkg_extracted.path }}/"
            dest: "{{ __hpc_azure_tests_dir }}/azurehpc-health-checks/"
            remote_src: true
            mode: preserve

        - name: Install dos2unix if not present
          package:
            name: dos2unix
            state: present
            use: "{{ (__hpc_server_is_ostree | d(false)) |
              ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

        - name: Find config files in triggerGHR/config
          find:
            paths: "{{ __hpc_azure_tests_dir }}/azurehpc-health-checks/triggerGHR/config"
            patterns: "*"
          register: __hpc_aznhc_config_files

        - name: Convert config files from DOS to Unix format
          command:
            cmd: dos2unix {{ item.path }}
          loop: "{{ __hpc_aznhc_config_files.files }}"
          changed_when: true

        - name: Make triggerGHR.sh executable
          file:
            path: "{{ __hpc_azure_tests_dir }}/azurehpc-health-checks/triggerGHR/triggerGHR.sh"
            mode: "0755"

        - name: Make pull-image-mcr.sh executable
          file:
            path: "{{ __hpc_azure_tests_dir }}/azurehpc-health-checks/dockerfile/pull-image-mcr.sh"
            mode: "0755"

        - name: Configure NCCL allreduce check for nd96isr_h200_v5
          replace:
            path: "{{ __hpc_azure_tests_dir }}/azurehpc-health-checks/conf/nd96isr_h200_v5.conf"
            regexp: '\* \|\| check_nccl_allreduce 431\.0 1 16G \$AZ_NHC_ROOT/topofiles/ndv5-topo\.xml'
            replace: '* || check_nccl_allreduce 431.0 1 16G'

        - name: Configure NCCL allreduce check for nd96isr_h100_v5
          replace:
            path: "{{ __hpc_azure_tests_dir }}/azurehpc-health-checks/conf/nd96isr_h100_v5.conf"
            regexp: '\* \|\| check_nccl_allreduce 460\.0 1 16G \$AZ_NHC_ROOT/topofiles/ndv5-topo\.xml'
            replace: '* || check_nccl_allreduce 460.0 1 16G'

        - name: Clean up temporary extraction directory
          file:
            path: "{{ __hpc_pkg_extracted.path }}"
            state: absent
          changed_when: false

    - name: Pull azurehpc-health-checks Docker image (always pull latest)
      when: not __hpc_skip_healthcheck | bool
      command:
        cmd: ./dockerfile/pull-image-mcr.sh cuda
        chdir: "{{ __hpc_azure_tests_dir }}/azurehpc-health-checks"
      changed_when: true

    - name: Install azurehpc-health-checks test script
      when: not __hpc_skip_healthcheck | bool
      template:
        src: test-azure-health-checks.sh.j2
        dest: "{{ __hpc_azure_tests_dir }}/test-azure-health-checks.sh"
        owner: root
        group: root
        mode: '0755'

- name: Install Azure HPC Diagnostics tool
  when: hpc_install_diagnostics
  block:
    - name: Check if Diagnostics are already installed
      stat:
        path: "{{ __hpc_azure_tools_dir }}/gather_azhpc_vm_diagnostics.sh"
      register: __hpc_azure_diags_installed

    - name: Download and install Diagnostics
      when: not __hpc_azure_diags_installed.stat.exists
      block:
        - name: Install dependencies
          package:
            name: "{{ __hpc_azure_diagnostics_packages }}"
            state: present
            use: "{{ (__hpc_server_is_ostree | d(false)) |
              ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
          register: __hpc_azure_diagnostics_packages_install
          until: __hpc_azure_diagnostics_packages_install is success

        - name: Download Diagnostics
          include_tasks: download_extract_package.yml
          vars:
            __hpc_pkg_info: "{{ __hpc_azhpc_diags_info }}"

        # The downloaded diagnostics script needs sufficient customisation that
        # the simplest way to do this is to patch it. However, we also have to
        # replace hard coded paths, so we need to be able to template it as
        # well.
        #
        # Templating can only occur on the control node, so we must first run
        # the patch through the template module and use the output as the patch
        # source. Then we can apply the patch to the remote extracted file and
        # copy it to the install location.
        - name: Create a temp file for the diagnostics patch
          tempfile:
            state: file
            prefix: hpc_diags
            suffix: .patch
          register: __hpc_diags_patch_file

        - name: Configure the diagnostics patch
          template:
            src: azhpc_vm_diagnostics.sh.patch.j2
            dest: "{{ __hpc_diags_patch_file.path }}"
            mode: '0644'

        - name: Patch Diagnostics script
          patch:
            src: "{{ __hpc_diags_patch_file.path }}"
            dest: "{{ __hpc_pkg_extracted.path }}/Linux/src/gather_azhpc_vm_diagnostics.sh"
            remote_src: true
            strip: 1

        - name: Install Diagnostics script
          copy:
            src: "{{ __hpc_pkg_extracted.path }}/Linux/src/gather_azhpc_vm_diagnostics.sh"
            dest: "{{ __hpc_azure_tools_dir }}/gather_azhpc_vm_diagnostics.sh"
            remote_src: true
            owner: root
            group: root
            mode: '0755'

        - name: Clean up temporary staging files
          file:
            path: "{{ item }}"
            state: absent
          loop:
            - "{{ __hpc_diags_patch_file.path }}"
            - "{{ __hpc_pkg_extracted.path }}"

- name: Clean dnf cache
  command: dnf clean all
  changed_when: false
