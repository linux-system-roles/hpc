# SPDX-License-Identifier: MIT
---
- name: Set platform/version specific variables
  include_tasks: tasks/set_vars.yml

- name: Deploy the GPG key for RHEL EPEL repository
  rpm_key:
    key: "{{ hpc_rhel_epel_rpm_key }}"
    state: present

# package dkms is required from this repo
- name: Install EPEL release package
  package:
    name: "{{ hpc_rhel_epel_rpm }}"
    state: present
    use: "{{ (__hpc_server_is_ostree | d(false)) |
      ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

- name: Deploy the GPG key for NVIDIA repositories
  rpm_key:
    key: "{{ hpc_nvidia_rpm_key }}"
    state: present

- name: Configure the NVIDIA CUDA repository
  yum_repository:
    name: nvidia-cuda
    description: NVIDIA CUDA repository
    baseurl: "{{ hpc_nvidia_cuda_repository }}"
    gpgcheck: true

- name: Configure storage
  when: hpc_manage_storage
  block:
    - name: Install lvm2 to get lvs command
      package:
        name: lvm2
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

    - name: Get current LV size of {{ hpc_rootlv_name }}
      vars:
        __hpc_lv: /dev/mapper/{{ hpc_rootvg_name }}-{{ hpc_rootlv_name }}
      command: lvs --noheadings --units g --nosuffix -o lv_size {{ __hpc_lv }}
      register: __hpc_rootlv_size_cmd
      changed_when: false

    - name: Get current LV size of {{ hpc_usrlv_name }}
      vars:
        __hpc_lv: /dev/mapper/{{ hpc_rootvg_name }}-{{ hpc_usrlv_name }}
      command: lvs --noheadings --units g --nosuffix -o lv_size {{ __hpc_lv }}
      register: __hpc_usrlv_size_cmd
      changed_when: false

    - name: Configure storage
      include_role:
        name: fedora.linux_system_roles.storage
      vars:
        hpc_rootlv_size_expected: >-
          {{ hpc_rootlv_size | regex_replace('[^0-9]', '') | int }}
        hpc_rootlv_size_curr: "{{ __hpc_rootlv_size_cmd.stdout | int }}"
        __hpc_rootlv_size: >-
          {{ (hpc_rootlv_size_expected | int > hpc_rootlv_size_curr | int)
          | ternary(hpc_rootlv_size, hpc_rootlv_size_curr ~ "G") }}
        hpc_usrlv_size_expected: >-
          {{ hpc_usrlv_size | regex_replace('[^0-9]', '') | int }}
        hpc_usrlv_size_curr: "{{ __hpc_usrlv_size_cmd.stdout | int }}"
        __hpc_usrlv_size: >-
          {{ (hpc_usrlv_size_expected | int > hpc_usrlv_size_curr | int)
          | ternary(hpc_usrlv_size, hpc_usrlv_size_curr ~ "G") }}
        storage_pools:
          - name: "{{ hpc_rootvg_name }}"
            grow_to_fill: true
            volumes:
              - name: "{{ hpc_rootlv_name }}"
                size: "{{ __hpc_rootlv_size }}"
                mount_point: "{{ hpc_rootlv_mount }}"
              - name: "{{ hpc_usrlv_name }}"
                size: "{{ __hpc_usrlv_size }}"
                mount_point: "{{ hpc_usrlv_mount }}"

# This is only needed for MicrosoftAzure VMs
- name: Update RHUI packages from Microsoft repositories
  package:
    name: "rhui*"
    state: latest  # noqa package-latest
    disablerepo: "*"
    enablerepo: "rhui-microsoft-*"
  when: ansible_system_vendor == "Microsoft Corporation"

- name: Force install kernel
  package:
    name: kernel-{{ __hpc_force_kernel_version }}
    state: present
  when: __hpc_force_kernel_version is not none
  notify: Reboot system

- name: >-
    Explicitly install kernel-devel and kernel-headers packages matching the
    currently running kernel
  package:
    name:
      - kernel-devel-{{ ansible_kernel }}
      - kernel-headers-{{ ansible_kernel }}
    state: present
  notify: Reboot system

- name: Flush handlers to apply new kernel
  meta: flush_handlers

- name: Ensure that dnf-command(versionlock) is installed
  package:
    name: dnf-command(versionlock)
    state: present

- name: Check if kernel versionlock entries exist
  stat:
    path: "{{ __hpc_versionlock_path }}"
  register: __hpc_versionlock_stat

- name: Get content of versionlock file
  command: cat {{ __hpc_versionlock_path }}
  register: __hpc_versionlock_content
  changed_when: false
  when: __hpc_versionlock_stat.stat.exists

# once nvidia drivers are built for a specific kernel version, we need to
# prevent installation of all kernel packages of a different version
# MS unsuccessfully tries to do this in azhpc-images build scripts:
# https://github.com/Azure/azhpc-images/blob/a3f92d283af6c9d11bf08eb0f8763ab67f7c8713/partners/rhel/common/install_utils.sh#L61
- name: Prevent installation of all kernel packages of a different version
  command: dnf versionlock add {{ item }}
  register: __hpc_versionlock_check
  changed_when: true
  when: >-
    not __hpc_versionlock_stat.stat.exists
    or __hpc_versionlock_content.stdout is not search(item  + "-[0-9]")
  loop: "{{ __hpc_versionlock_rpms }}"

- name: Enable proprietary nvidia-driver
  package:
    name: "{{ hpc_nvidia_driver_module }}"
    state: present
    use: "{{ (__hpc_server_is_ostree | d(false)) |
      ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
  notify: Reboot system

- name: Install CUDA driver and enable nvidia-persistenced.service
  when: hpc_install_cuda_driver
  block:
    - name: Install CUDA driver
      package:
        name: "{{ hpc_cuda_driver_packages }}"
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

    - name: Enable nvidia-persistenced.service
      service:
        name: nvidia-persistenced.service
        enabled: true

- name: Install CUDA Toolkit
  when: hpc_install_cuda_toolkit
  package:
    name: "{{ hpc_cuda_toolkit_packages }}"
    state: present
    use: "{{ (__hpc_server_is_ostree | d(false)) |
      ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
  register: __hpc_install_cuda_toolkit
  until: __hpc_install_cuda_toolkit is success

- name: Install NVIDIA NCCL
  when: hpc_install_hpc_nvidia_nccl
  package:
    name: "{{ hpc_nvidia_nccl_packages }}"
    state: present
    use: "{{ (__hpc_server_is_ostree | d(false)) |
      ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

- name: Install NVIDIA Fabric Manager
  when: hpc_install_nvidia_fabric_manager
  package:
    name: "{{ hpc_nvidia_fabric_manager_packages }}"
    state: present
    use: "{{ (__hpc_server_is_ostree | d(false)) |
      ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

- name: Enable Fabric Manager service
  service:
    name: nvidia-fabricmanager
    state: started
    enabled: true

- name: Install NVIDIA RDMA packages
  when: hpc_install_nvidia_rdma
  package:
    name: "{{ hpc_nvidia_rdma_packages }}"
    state: present
    use: "{{ (__hpc_server_is_ostree | d(false)) |
      ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

# azhpc-images build scripts do the same thing here:
# https://github.com/Azure/azhpc-images/blob/a3f92d283af6c9d11bf08eb0f8763ab67f7c8713/common/install_waagent.sh#L53
- name: Enable RDMA in waagent configuration
  when: hpc_install_nvidia_rdma
  lineinfile:
    path: /etc/waagent.conf
    line: "OS.EnableRDMA=y"
    create: true
    backup: true
    mode: "0644"
  notify: Restart waagent

- name: Let user's applications lock as much memory as they need
  lineinfile:
    path: /etc/security/limits.conf
    line: "{{ item }}"
    create: true
    backup: true
    mode: "0644"
  loop:
    - "* soft memlock unlimited"
    - "* hard memlock unlimited"

- name: Install openmpi
  when: hpc_install_openmpi
  package:
    name: "{{ hpc_openmpi_packages }}"
    state: present
    use: "{{ (__hpc_server_is_ostree | d(false)) |
      ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
