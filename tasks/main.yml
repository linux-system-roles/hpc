# SPDX-License-Identifier: MIT
---
- name: Set platform/version specific variables
  include_tasks: tasks/set_vars.yml

- name: Fail on unsupported architectures
  fail:
    msg: >-
      This role supports only on x86_64 architecture.
      You are running on {{ ansible_facts['architecture'] }} architecture.
      Let us know if you need the role to support it.
  when: ansible_facts['architecture'] != 'x86_64'

- name: Fail if role installs openmpi without cuda toolkit
  fail:
    msg:
      - Building OpenMPI requires multiple packages to be installed
      - You must set the following variables true to build OpenMPI with Nvidia
      - GPU support
      - "hpc_install_cuda_toolkit: true"
      - "hpc_install_nvidia_nccl: true"
  when:
    - hpc_build_openmpi_w_nvidia_gpu_support
    - not hpc_install_cuda_toolkit
    - not hpc_install_nvidia_nccl

- name: Deploy GPG keys for repositories
  rpm_key:
    key: "{{ item.key }}"
    state: present
  loop:
    - "{{ __hpc_rhel_epel_repo }}"
    - "{{ __hpc_nvidia_cuda_repo }}"
    - "{{ __hpc_microsoft_prod_repo }}"

# package dkms is required from this repo
- name: Install EPEL release package
  package:
    name: "{{ __hpc_rhel_epel_repo.rpm }}"
    state: present
    use: "{{ (__hpc_server_is_ostree | d(false)) |
      ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

- name: Configure repositories
  yum_repository:
    name: "{{ item.name }}"
    description: "{{ item.description }}"
    baseurl: "{{ item.baseurl }}"
    gpgcheck: true
  loop:
    - "{{ __hpc_nvidia_cuda_repo }}"
    - "{{ __hpc_microsoft_prod_repo }}"

- name: Replace default RHUI Azure repository with the EUS repository
  when: hpc_enable_eus_repo
  block:
    - name: Get list of installed repositories
      command: dnf repolist
      changed_when: false
      register: __hpc_dnf_repolist

    - name: Ensure that the non-EUS RHUI Azure repository is not installed
      package:
        name: rhui-azure-rhel{{ ansible_facts['distribution_major_version'] }}
        state: absent
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

    - name: Enable the RHUI Azure EUS repository
      when: >-
        'rhui-microsoft-azure-rhel' + ansible_facts['distribution_major_version']
        + '-eus ' not in __hpc_dnf_repolist.stdout
      block:
        - name: Create a temp file for the EUS repository configuration
          tempfile:
            state: file
            prefix: rhel{{ ansible_facts['distribution_major_version'] }}-eus
            suffix: .config
          register: __hpc_euc_config

        - name: Generate the repository configuration template
          template:
            src: rhel-ver-eus.config
            dest: "{{ __hpc_euc_config.path }}"
            mode: "0644"
            owner: root
            group: root

        - name: Add EUS repository
          command: >-
            dnf --config {{ __hpc_euc_config.path }} install
            rhui-azure-rhel{{ ansible_facts['distribution_major_version'] }}-eus
            --assumeyes
          changed_when: true

        - name: Lock the RHEL minor release to the current minor release
          copy:
            content: "{{ ansible_facts['distribution_version'] }}"
            dest: /etc/dnf/vars/releasever
            mode: "0644"
            owner: root
            group: root

- name: Configure firewall to use trusted zone as default
  when: hpc_manage_firewall
  include_role:
    name: fedora.linux_system_roles.firewall
  vars:
    firewall:
      - set_default_zone: trusted
        state: enabled

- name: Configure storage
  when: hpc_manage_storage
  block:
    - name: Install lvm2 to get lvs command
      package:
        name: lvm2
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

    - name: Get current LV size of {{ hpc_rootlv_name }}
      vars:
        __hpc_lv: /dev/mapper/{{ hpc_rootvg_name }}-{{ hpc_rootlv_name }}
      command: lvs --noheadings --units g --nosuffix -o lv_size {{ __hpc_lv }}
      register: __hpc_rootlv_size_cmd
      changed_when: false

    - name: Get current LV size of {{ hpc_usrlv_name }}
      vars:
        __hpc_lv: /dev/mapper/{{ hpc_rootvg_name }}-{{ hpc_usrlv_name }}
      command: lvs --noheadings --units g --nosuffix -o lv_size {{ __hpc_lv }}
      register: __hpc_usrlv_size_cmd
      changed_when: false

    - name: Configure storage
      include_role:
        name: fedora.linux_system_roles.storage
      vars:
        hpc_rootlv_size_expected: >-
          {{ hpc_rootlv_size | regex_replace('[^0-9]', '') | int }}
        hpc_rootlv_size_curr: "{{ __hpc_rootlv_size_cmd.stdout | int }}"
        __hpc_rootlv_size: >-
          {{ (hpc_rootlv_size_expected | int > hpc_rootlv_size_curr | int)
          | ternary(hpc_rootlv_size, hpc_rootlv_size_curr ~ "G") }}
        hpc_usrlv_size_expected: >-
          {{ hpc_usrlv_size | regex_replace('[^0-9]', '') | int }}
        hpc_usrlv_size_curr: "{{ __hpc_usrlv_size_cmd.stdout | int }}"
        __hpc_usrlv_size: >-
          {{ (hpc_usrlv_size_expected | int > hpc_usrlv_size_curr | int)
          | ternary(hpc_usrlv_size, hpc_usrlv_size_curr ~ "G") }}
        storage_pools:
          - name: "{{ hpc_rootvg_name }}"
            grow_to_fill: true
            volumes:
              - name: "{{ hpc_rootlv_name }}"
                size: "{{ __hpc_rootlv_size }}"
                mount_point: "{{ hpc_rootlv_mount }}"
              - name: "{{ hpc_usrlv_name }}"
                size: "{{ __hpc_usrlv_size }}"
                mount_point: "{{ hpc_usrlv_mount }}"

- name: Force install kernel version
  package:
    name: kernel-{{ __hpc_force_kernel_version }}
    state: present
    allow_downgrade: true
  when: __hpc_force_kernel_version is not none
  notify: Reboot system

- name: Update kernel
  when:
    - hpc_update_kernel
    - __hpc_force_kernel_version is none
  package:
    name: kernel
    state: latest  # noqa package-latest
  notify: Reboot system

- name: Get package facts
  package_facts:
  no_log: true

# This is required for dkms to build NVidia drivers for all kernels
- name: Install kernel-devel and kernel-headers packages for all kernels
  vars:
    kernel_version: "{{ item.version }}-{{ item.release }}"
  package:
    name:
      - kernel-devel-{{ kernel_version }}
      - kernel-headers-{{ kernel_version }}
    state: present
  notify: Reboot system
  loop: "{{ ansible_facts.packages.kernel }}"

- name: Ensure that dnf-command(versionlock) is installed
  package:
    name: dnf-command(versionlock)
    state: present

- name: Check if kernel versionlock entries exist
  stat:
    path: "{{ __hpc_versionlock_path }}"
  register: __hpc_versionlock_stat

# once nvidia drivers are built for a specific kernel version, we need to
# prevent installation of all kernel packages of a different version
# MS unsuccessfully tries to do this in azhpc-images build scripts:
# https://github.com/Azure/azhpc-images/blob/a3f92d283af6c9d11bf08eb0f8763ab67f7c8713/partners/rhel/common/install_utils.sh#L61
- name: Prevent installation of all kernel packages of a different version
  command: dnf versionlock add {{ item }}
  register: __hpc_versionlock_check
  changed_when: >-
    'Package already locked in equivalent form'
    not in __hpc_versionlock_check.stdout
  loop: "{{ __hpc_kernel_versionlock_rpms }}"

- name: Update all packages to bring system to the latest state
  when: hpc_update_all_packages
  package:
    name: "*"
    state: latest  # noqa package-latest

- name: Install NVidia driver
  # Note that currently the role supports only Microsoft Azure
  # When we add more cloud providers, we need to update this condition
  when: ansible_facts["system_vendor"] == "Microsoft Corporation"
  block:
    - name: Get list of dnf modules
      command: dnf module list
      register: __hpc_dnf_modules
      changed_when: false

    - name: Reset nvidia-driver module if it is enabled of different version
      when: __hpc_dnf_modules.stdout | regex_search('nvidia-driver (?!575-dkms).* \[e\]')
      command: dnf module reset nvidia-driver --assumeyes
      changed_when: true

    - name: Enable NVIDIA driver module
      vars:
        nvidia_enabled_pattern: >-
          {{ __hpc_nvidia_driver_module | regex_replace(':', ' ') + ' \[e\]' }}
      when: __hpc_dnf_modules.stdout is not search(nvidia_enabled_pattern)
      command: dnf module enable {{ __hpc_nvidia_driver_module }} --assumeyes
      notify: Reboot system
      changed_when: true

    - name: Install dkms
      package:
        name: "{{ __hpc_dkms_packages }}"
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_dkms_packages_install
      until: __hpc_dkms_packages_install is success

    - name: Install NVIDIA driver
      package:
        name: "{{ __hpc_nvidia_driver_packages }}"
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_nvidia_driver_packages_install
      until: __hpc_nvidia_driver_packages_install is success

    # This makes the role not idempotent.
    # We need to find a condition in which starting and enabling is not enough.
    - name: Restart dkms service to make it build nvidia drivers for all kernels
      service:
        name: dkms.service
        enabled: true
        state: restarted

- name: Install CUDA driver and enable nvidia-persistenced.service
  when:
    - ansible_facts["system_vendor"] == "Microsoft Corporation"
    - hpc_install_cuda_driver
  block:
    - name: Install CUDA driver
      package:
        name: "{{ __hpc_cuda_driver_packages }}"
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_cuda_driver_packages_install
      until: __hpc_cuda_driver_packages_install is success

    - name: Enable nvidia-persistenced.service
      service:
        name: nvidia-persistenced.service
        enabled: true

- name: Install CUDA Toolkit and lock version of its packages
  when:
    - hpc_install_cuda_toolkit
    - ansible_facts["system_vendor"] == "Microsoft Corporation"
  block:
    - name: Install CUDA Toolkit
      package:
        name: "{{ __hpc_cuda_toolkit_packages }}"
        state: present
        allow_downgrade: true
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_cuda_toolkit_packages_install
      until: __hpc_cuda_toolkit_packages_install is success

    - name: Prevent update of CUDA Toolkit packages
      command: dnf versionlock add {{ item }}
      register: __hpc_versionlock_check
      changed_when: >-
        'Package already locked in equivalent form'
        not in __hpc_versionlock_check.stdout
      loop: "{{ __hpc_kernel_versionlock_rpms }}"

- name: Install NVIDIA NCCL and lock version of its packages
  when: hpc_install_hpc_nvidia_nccl
  block:
    - name: Install NVIDIA NCCL
      package:
        name: "{{ __hpc_nvidia_nccl_packages }}"
        state: present
        allow_downgrade: true
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_nvidia_nccl_packages_install
      until: __hpc_nvidia_nccl_packages_install is success

    - name: Prevent update of NVIDIA NCCL packages
      command: dnf versionlock add {{ item }}
      register: __hpc_versionlock_check
      changed_when: >-
        'Package already locked in equivalent form'
        not in __hpc_versionlock_check.stdout
      loop: "{{ __hpc_nvidia_nccl_packages }}"

- name: Install NVIDIA Fabric Manager and enable service
  when: hpc_install_nvidia_fabric_manager
  block:
    - name: Install NVIDIA Fabric Manager
      package:
        name: "{{ __hpc_nvidia_fabric_manager_packages }}"
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_nvidia_fabric_manager_packages_install
      until: __hpc_nvidia_fabric_manager_packages_install is success

    - name: Ensure that Fabric Manager service is enabled
      service:
        name: nvidia-fabricmanager
        enabled: true

- name: Install RDMA packages
  when: hpc_install_rdma
  block:
    - name: Install RDMA packages
      package:
        name: "{{ __hpc_rdma_packages }}"
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_rdma_packages_install
      until: __hpc_rdma_packages_install is success

    # azhpc-images build scripts do the same thing here:
    # https://github.com/Azure/azhpc-images/blob/a3f92d283af6c9d11bf08eb0f8763ab67f7c8713/common/install_waagent.sh#L53
    - name: Enable RDMA in waagent configuration
      lineinfile:
        path: /etc/waagent.conf
        line: "OS.EnableRDMA=y"
        create: true
        backup: true
        mode: "0644"
      notify: Restart waagent

- name: Install common OpenMPI packages
  when: hpc_install_system_openmpi or hpc_build_openmpi_w_nvidia_gpu_support
  package:
    name: "{{ __hpc_openmpi_common_packages }}"
    state: present
    use: "{{ (__hpc_server_is_ostree | d(false)) |
      ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
  register: __hpc_openmpi_common_packages_install
  until: __hpc_openmpi_common_packages_install is success

- name: Install system OpenMPI
  when: hpc_install_system_openmpi
  package:
    name: "{{ __hpc_system_openmpi_packages }}"
    state: present
    use: "{{ (__hpc_server_is_ostree | d(false)) |
      ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
  register: __hpc_system_openmpi_packages_install
  until: __hpc_system_openmpi_packages_install is success

- name: Download, build, and configure OpenMPI and all required packages
  when: hpc_build_openmpi_w_nvidia_gpu_support
  block:
    - name: Install build dependencies
      package:
        name: >-
          {{ __hpc_pmix_build_dependencies
          + __hpc_gdrcopy_build_dependencies
          + __hpc_openmpi_build_dependencies }}
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

    # Must keep the original tarball name because it's hardcoded in hpcx
    - name: Set __hpc_hpcx_path fact
      vars:
        hpcx_tarball_basename: "{{ __hpc_hpcx_info.url | basename
          | regex_replace('\\.[^.]*$', '') }}"
      set_fact:
        __hpc_hpcx_path: "{{ __hpc_install_prefix }}/{{ hpcx_tarball_basename }}"

    - name: Set facts for building HPC-X and OpenMPI
      set_fact:
        __hpc_hpcx_rebuild_path: "{{ __hpc_hpcx_path }}/hpcx-rebuild"
        __hpc_hcoll_path: "{{ __hpc_hpcx_path }}/hcoll"
        __hpc_ucx_path: "{{ __hpc_hpcx_path }}/ucx/hpcx-rebuild"
        __hpc_ucc_path: "{{ __hpc_hpcx_path }}/ucc"
        __hpc_pmix_path: "{{ __hpc_install_prefix }}/{{ __hpc_pmix_info.name }}/{{ __hpc_pmix_info.version }}"
        __hpc_openmpi_path: "{{ __hpc_install_prefix }}/{{ __hpc_openmpi_info.name }}-{{ __hpc_openmpi_info.version }}"
        __hpc_cuda_path: /usr/local/cuda

    - name: Get stat of pmix path
      stat:
        path: "{{ __hpc_pmix_path }}/bin/pmixcc"
      register: __hpc_pmix_path_stat

    - name: Download and build PMIx
      when: not __hpc_pmix_path_stat.stat.exists
      block:
        - name: Download PMIx
          include_tasks: tasks/download_extract_package.yml
          vars:
            __hpc_pkg_info: "{{ __hpc_pmix_info }}"

        - name: Build PMIx
          command:
            cmd: "{{ item }}"
            chdir: "{{ __hpc_pkg_extracted.path }}"
          changed_when: true
          loop:
            - >-
              ./configure --prefix={{ __hpc_pmix_path }}
              --enable-pmix-binaries
              --disable-dependency-tracking
            - make -j {{ ansible_facts["processor_nproc"] }}
            - make install

    - name: Ensure PMIx modulefile directory exists
      file:
        path: "{{ __hpc_module_dir }}/pmix"
        state: directory
        owner: root
        group: root
        mode: '0755'

    # The openmpi-{{ __hpc_openmpi_info.version }} env module depends on this
    - name: Install PMIx modulefile
      template:
        src: pmix-ver.lua
        dest: "{{ __hpc_module_dir }}/pmix/pmix-{{ __hpc_pmix_info.version }}.lua"
        owner: root
        group: root
        mode: '0755'

    - name: Install GDRCopy packages
      vars:
        gdrcopy_version: "{{ __hpc_gdrcopy_info.version | split('-') | first }}"
      when: >-
        (ansible_facts.packages.gdrcopy is not defined
        or ansible_facts.packages.gdrcopy[0].version != gdrcopy_version )
        or (ansible_facts.packages["gdrcopy-kmod"] is not defined
        or ansible_facts.packages["gdrcopy-kmod"][0].version != gdrcopy_version)
        or (ansible_facts.packages["gdrcopy-devel"] is not defined
        or ansible_facts.packages["gdrcopy-devel"][0].version != gdrcopy_version)
      block:
        - name: Download GDRCopy
          include_tasks: tasks/download_extract_package.yml
          vars:
            __hpc_pkg_info: "{{ __hpc_gdrcopy_info }}"

        - name: Build GDRCopy RPM packages
          environment:
            CUDA: "{{ __hpc_cuda_path }}"
          command:
            cmd: packages/build-rpm-packages.sh
            chdir: "{{ __hpc_pkg_extracted.path }}"
          changed_when: true

        - name: Install GDRCopy packages from built RPMs
          package:
            name:
              - "{{ __hpc_pkg_extracted.path }}/gdrcopy-kmod-{{ __hpc_gdrcopy_info.version }}dkms.{{ __hpc_gdrcopy_info.distribution }}.noarch.rpm"
              - "{{ __hpc_pkg_extracted.path }}/gdrcopy-{{ __hpc_gdrcopy_info.version }}.{{ __hpc_gdrcopy_info.distribution }}.x86_64.rpm"
              - "{{ __hpc_pkg_extracted.path }}/gdrcopy-devel-{{ __hpc_gdrcopy_info.version }}.{{ __hpc_gdrcopy_info.distribution }}.noarch.rpm"
            disable_gpg_check: true
            state: present
            use: "{{ (__hpc_server_is_ostree | d(false)) |
              ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

        - name: Remove extracted tarball
          file:
            path: "{{ __hpc_pkg_extracted.path }}"
            state: absent
          changed_when: false

    - name: Get stat of hpcx-rebuild path
      stat:
        path: "{{ __hpc_hpcx_rebuild_path }}"
      register: __hpc_hpcx_rebuild_path_stat

    - name: Download and build HPC-X
      when: not __hpc_hpcx_rebuild_path_stat.stat.exists
      block:
        - name: Download HPC-X
          include_tasks: tasks/download_extract_package.yml
          vars:
            __hpc_pkg_info: "{{ __hpc_hpcx_info }}"

        # Packages are installed in /opt, not "/build-result", so pkgconfig
        # files used by openmpi's cmake configure scripts need to be updated.
        - name: Ensure that pkgconfig files use hpcx_home={{ __hpc_hpcx_path }}
          replace:
            path: "{{ item }}"
            regexp: "/build-result/"
            replace: "{{ __hpc_install_prefix }}/"
            backup: true
          loop:
            - "{{ __hpc_pkg_extracted.path }}/hcoll/lib/pkgconfig/hcoll.pc"
            - "{{ __hpc_pkg_extracted.path }}/ucc/lib/pkgconfig/ucc.pc"
            - "{{ __hpc_pkg_extracted.path }}/ucx/lib/pkgconfig/ucx.pc"

        # the shipped pkgconfig for hcoll only includes -lhcoll. However, this
        # library depends on -locoms, so we have to add that to prevent openmpi
        # from failing to link against libhcoll.
        - name: Update hcoll pkgconfig file to add -locoms parameter
          replace:
            path: "{{ __hpc_pkg_extracted.path }}/hcoll/lib/pkgconfig/hcoll.pc"
            regexp: "-lhcoll"
            replace: "-lhcoll -locoms"
            backup: true

        - name: Copy HPC-X files to {{ __hpc_hpcx_path }}
          copy:
            src: "{{ __hpc_pkg_extracted.path }}/"
            remote_src: true
            dest: "{{ __hpc_hpcx_path }}"
            mode: "0755"
            owner: root
            group: root

        - name: Rebuild HPC-X with PMIx
          environment:
            CUDA_HOME: "{{ __hpc_cuda_path }}"
          command: >-
            {{ __hpc_hpcx_path }}/utils/hpcx_rebuild.sh
            --with-hcoll
            --cuda
            --rebuild-ucx
            --ompi-extra-config
            '--with-pmix={{ __hpc_pmix_path }}
            --enable-orterun-prefix-by-default'
          changed_when: true

        - name: Copy ompi/tests to hpcx-rebuild in {{ __hpc_hpcx_path }}
          copy:
            src: "{{ __hpc_hpcx_path }}/ompi/tests/"
            remote_src: true
            dest: "{{ __hpc_hpcx_rebuild_path }}"
            mode: "0755"
            owner: root
            group: root

        - name: Remove extracted tarball
          file:
            path: "{{ __hpc_pkg_extracted.path }}"
            state: absent
          changed_when: false

    - name: Ensure MPI modulefile directory exists
      file:
        path: "{{ __hpc_module_dir }}/mpi"
        state: directory
        owner: root
        group: root
        mode: '0755'

    - name: Install NVidia HPCX OpemMPI modulefile
      template:
        src: hpcx-ver.lua
        dest: >-
          {{ __hpc_module_dir }}/mpi/hpcx-{{ __hpc_hpcx_info.version }}.lua
        owner: root
        group: root
        mode: '0755'

    - name: Install NVidia HPCX OpemMPI with PMIx {{ __hpc_pmix_info.version }}
      template:
        src: hpcx-ver-pmix-ver.lua
        dest: >-
          {{ __hpc_module_dir }}/mpi/hpcx-{{
          __hpc_hpcx_info.version }}-pmix-{{ __hpc_pmix_info.version }}.lua
        owner: root
        group: root
        mode: '0755'

    - name: Get stat of openmpi path
      stat:
        path: "{{ __hpc_openmpi_path }}"
      register: __hpc_openmpi_path_stat

    - name: Download and build OpenMPI
      when: not __hpc_openmpi_path_stat.stat.exists
      block:
        - name: Download {{ __hpc_openmpi_info.name }}
          include_tasks: tasks/download_extract_package.yml
          vars:
            __hpc_pkg_info: "{{ __hpc_openmpi_info }}"

        - name: Build {{ __hpc_openmpi_info.name }}
          command:
            cmd: "{{ item }}"
            chdir: "{{ __hpc_pkg_extracted.path }}"
          changed_when: true
          loop:
            - >-
              ./configure --prefix={{ __hpc_openmpi_path }}
              --with-ucx={{ __hpc_ucx_path }}
              --with-ucc={{ __hpc_ucc_path }}
              --with-hcoll={{ __hpc_hcoll_path }}
              --with-pmix={{ __hpc_pmix_path }}
              --enable-prte-prefix-by-default
              --with-platform=contrib/platform/mellanox/optimized
              --with-cuda={{ __hpc_cuda_path }}
            - make -j {{ ansible_facts["processor_nproc"] }}
            - make install

        - name: Remove extracted tarball
          file:
            path: "{{ __hpc_pkg_extracted.path }}"
            state: absent
          changed_when: false

    - name: Install OpenMPI modulefile
      template:
        src: openmpi-ver-cuda12-gpu.lua
        dest: "{{ __hpc_module_dir }}/mpi/openmpi-{{ __hpc_openmpi_info.version }}-cuda12-gpu.lua"
        owner: root
        group: root
        mode: '0755'

- name: Tune system for HPC
  when: hpc_tuning
  block:
    - name: Remove user memory limits to ensure applications aren't restricted
      template:
        src: 90-hpc-limits.conf
        dest: /etc/security/limits.d/
        owner: root
        group: root
        mode: "0644"

    - name: Add sysctl tuning configuration for HPC
      template:
        src: 90-hpc-sysctl.conf
        dest: /etc/sysctl.d/
        owner: root
        group: root
        mode: '0644'
      notify: Reload sysctl

    - name: Load sunrpc kernel module
      lineinfile:
        path: /etc/modules-load.d/sunrpc.conf
        line: sunrpc
        create: true
        owner: root
        group: root
        mode: '0644'
      notify: Restart systemd-modules-load

    - name: Check if sunrpc module is loaded
      command: lsmod
      register: __hpc_loaded_modules
      changed_when: false

    - name: Load sunrpc module if not loaded
      when: "'sunrpc' not in __hpc_loaded_modules.stdout"
      command: modprobe sunrpc
      changed_when: true

    - name: Copy NFS readahead udev rules for Azure infrastructure
      template:
        src: 90-nfs-readahead.rules
        dest: /etc/udev/rules.d/
        owner: root
        group: root
        mode: '0644'
      notify: Reload udev

- name: Remove build dependencies
  vars:
    __hpc_dependencies: >-
      {{ __hpc_gdrcopy_build_dependencies
      + __hpc_pmix_build_dependencies
      + __hpc_openmpi_build_dependencies }}
  package:
    name: "{{ __hpc_dependencies }}"
    state: absent
    use: "{{ (__hpc_server_is_ostree | d(false)) |
      ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

- name: Clean dnf cache
  command: dnf clean all
  changed_when: false
