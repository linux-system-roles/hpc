# SPDX-License-Identifier: MIT
---
- name: Set platform/version specific variables
  include_tasks: tasks/set_vars.yml

- name: Fail on unsupported architectures
  fail:
    msg: >-
      This role supports only on x86_64 architecture.
      You are running on {{ ansible_facts['architecture'] }} architecture.
      Let us know if you need the role to support it.
  when: ansible_facts['architecture'] != 'x86_64'

- name: Fail if role installs openmpi without cuda toolkit
  fail:
    msg:
      - Building OpenMPI requires multiple packages to be installed
      - You must set the following variables true to build OpenMPI with Nvidia
      - GPU support
      - "hpc_install_cuda_toolkit: true"
      - "hpc_install_nvidia_nccl: true"
  when:
    - hpc_build_openmpi_w_nvidia_gpu_support
    - not hpc_install_cuda_toolkit
    - not hpc_install_nvidia_nccl

- name: Fail if role installs NVIDIA Container Toolkit without Docker
  fail:
    msg:
      - NVIDIA Container Toolkit requires Docker to be installed
      - You must set the following variable to true
      - "hpc_install_docker: true"
  when:
    - hpc_install_nvidia_container_toolkit
    - not hpc_install_docker

- name: Deploy GPG keys for repositories
  rpm_key:
    key: "{{ item.key }}"
    state: present
  loop:
    - "{{ __hpc_rhel_epel_repo }}"
    - "{{ __hpc_nvidia_cuda_repo }}"
    - "{{ __hpc_microsoft_prod_repo }}"

# package dkms is required from this repo
- name: Install EPEL release package
  package:
    name: "{{ __hpc_rhel_epel_repo.rpm }}"
    state: present
    use: "{{ (__hpc_server_is_ostree | d(false)) |
      ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

- name: Configure repositories
  yum_repository:
    name: "{{ item.name }}"
    description: "{{ item.description }}"
    baseurl: "{{ item.baseurl }}"
    gpgcheck: true
  loop:
    - "{{ __hpc_nvidia_cuda_repo }}"
    - "{{ __hpc_microsoft_prod_repo }}"

- name: Replace default RHUI Azure repository with the EUS repository
  when: hpc_enable_eus_repo
  block:
    - name: Get list of installed repositories
      command: dnf repolist
      changed_when: false
      register: __hpc_dnf_repolist

    - name: Ensure that the non-EUS RHUI Azure repository is not installed
      package:
        name: rhui-azure-rhel{{ ansible_facts['distribution_major_version'] }}
        state: absent
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

    - name: Enable the RHUI Azure EUS repository
      when: >-
        'rhui-microsoft-azure-rhel' + ansible_facts['distribution_major_version']
        + '-eus ' not in __hpc_dnf_repolist.stdout
      block:
        - name: Create a temp file for the EUS repository configuration
          tempfile:
            state: file
            prefix: rhel{{ ansible_facts['distribution_major_version'] }}-eus
            suffix: .config
          register: __hpc_euc_config

        - name: Generate the repository configuration template
          template:
            src: rhel-ver-eus.config
            dest: "{{ __hpc_euc_config.path }}"
            mode: "0644"
            owner: root
            group: root

        - name: Add EUS repository
          command: >-
            dnf --config {{ __hpc_euc_config.path }} install
            rhui-azure-rhel{{ ansible_facts['distribution_major_version'] }}-eus
            --assumeyes
          changed_when: true

        - name: Lock the RHEL minor release to the current minor release
          copy:
            content: "{{ ansible_facts['distribution_version'] }}"
            dest: /etc/dnf/vars/releasever
            mode: "0644"
            owner: root
            group: root

- name: Configure firewall to use trusted zone as default
  when: hpc_manage_firewall
  include_role:
    name: fedora.linux_system_roles.firewall
  vars:
    firewall:
      - set_default_zone: trusted
        state: enabled

- name: Configure storage
  when: hpc_manage_storage
  block:
    - name: Validate storage size formats
      assert:
        that:
          - hpc_rootlv_size is match('^[0-9]+G$')
          - hpc_usrlv_size is match('^[0-9]+G$')
          - hpc_varlv_size is match('^[0-9]+G$')
        fail_msg: |
          Storage size variables should be whole numbers in format <number>G (e.g., '15G', not '15.5G').
          Got: rootlv={{ hpc_rootlv_size }}, usrlv={{ hpc_usrlv_size }}, varlv={{ hpc_varlv_size }}

    - name: Install lvm2 to get lvs command
      package:
        name: lvm2
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

    - name: Check if rootlv exists
      stat:
        path: /dev/mapper/{{ hpc_rootvg_name }}-{{ hpc_rootlv_name }}
      register: __hpc_rootlv_stat

    - name: Check if usrlv exists
      stat:
        path: /dev/mapper/{{ hpc_rootvg_name }}-{{ hpc_usrlv_name }}
      register: __hpc_usrlv_stat

    - name: Check if varlv exists
      stat:
        path: /dev/mapper/{{ hpc_rootvg_name }}-{{ hpc_varlv_name }}
      register: __hpc_varlv_stat

    - name: Get current LV size of rootlv
      command: lvs --noheadings --units g --nosuffix -o lv_size /dev/mapper/{{ hpc_rootvg_name }}-{{ hpc_rootlv_name }}
      register: __hpc_rootlv_size_cmd
      changed_when: false
      when: __hpc_rootlv_stat.stat.exists

    - name: Get current LV size of usrlv
      command: lvs --noheadings --units g --nosuffix -o lv_size /dev/mapper/{{ hpc_rootvg_name }}-{{ hpc_usrlv_name }}
      register: __hpc_usrlv_size_cmd
      changed_when: false
      when: __hpc_usrlv_stat.stat.exists

    - name: Get current LV size of varlv
      command: lvs --noheadings --units g --nosuffix -o lv_size /dev/mapper/{{ hpc_rootvg_name }}-{{ hpc_varlv_name }}
      register: __hpc_varlv_size_cmd
      changed_when: false
      when: __hpc_varlv_stat.stat.exists

    - name: Initialize volumes list
      set_fact:
        __hpc_volumes: []

    - name: Add rootlv if exists and needs expansion
      set_fact:
        __hpc_volumes: "{{ __hpc_volumes + [volume_config] }}"
      vars:
        size_expected: "{{ hpc_rootlv_size | regex_replace('[^0-9]', '') | int }}"
        size_current: "{{ __hpc_rootlv_size_cmd.stdout | default('0') | trim | int }}"
        volume_config:
          name: "{{ hpc_rootlv_name }}"
          size: "{{ hpc_rootlv_size }}"
          mount_point: "{{ hpc_rootlv_mount }}"
      when:
        - __hpc_rootlv_stat.stat.exists
        - size_expected > size_current

    - name: Add usrlv if exists and needs expansion
      set_fact:
        __hpc_volumes: "{{ __hpc_volumes + [volume_config] }}"
      vars:
        size_expected: "{{ hpc_usrlv_size | regex_replace('[^0-9]', '') | int }}"
        size_current: "{{ __hpc_usrlv_size_cmd.stdout | default('0') | trim | int }}"
        volume_config:
          name: "{{ hpc_usrlv_name }}"
          size: "{{ hpc_usrlv_size }}"
          mount_point: "{{ hpc_usrlv_mount }}"
      when:
        - __hpc_usrlv_stat.stat.exists
        - size_expected > size_current

    - name: Add varlv if exists and needs expansion
      set_fact:
        __hpc_volumes: "{{ __hpc_volumes + [volume_config] }}"
      vars:
        size_expected: "{{ hpc_varlv_size | regex_replace('[^0-9]', '') | int }}"
        size_current: "{{ __hpc_varlv_size_cmd.stdout | default('0') | trim | int }}"
        volume_config:
          name: "{{ hpc_varlv_name }}"
          size: "{{ hpc_varlv_size }}"
          mount_point: "{{ hpc_varlv_mount }}"
      when:
        - __hpc_varlv_stat.stat.exists
        - size_expected > size_current

    - name: Configure storage
      include_role:
        name: fedora.linux_system_roles.storage
      vars:
        storage_pools:
          - name: "{{ hpc_rootvg_name }}"
            grow_to_fill: true
            volumes: "{{ __hpc_volumes }}"
      when: __hpc_volumes | length > 0

- name: Force install kernel version
  package:
    name: kernel-{{ __hpc_force_kernel_version }}
    state: present
    allow_downgrade: true
  when: __hpc_force_kernel_version is not none
  notify: Reboot system

- name: Update kernel
  when:
    - hpc_update_kernel
    - __hpc_force_kernel_version is none
  package:
    name: kernel
    state: latest  # noqa package-latest
  notify: Reboot system

- name: Get package facts
  package_facts:
  no_log: true

# This is required for dkms to build NVidia drivers for all kernels
- name: Install kernel-devel and kernel-headers packages for all kernels
  vars:
    kernel_version: "{{ item.version }}-{{ item.release }}"
  package:
    name:
      - kernel-devel-{{ kernel_version }}
      - kernel-headers-{{ kernel_version }}
    state: present
  notify: Reboot system
  loop: "{{ ansible_facts.packages.kernel }}"

- name: Ensure that dnf-command(versionlock) is installed
  package:
    name: dnf-command(versionlock)
    state: present

- name: Check if kernel versionlock entries exist
  stat:
    path: "{{ __hpc_versionlock_path }}"
  register: __hpc_versionlock_stat

# once nvidia drivers are built for a specific kernel version, we need to
# prevent installation of all kernel packages of a different version
# MS unsuccessfully tries to do this in azhpc-images build scripts:
# https://github.com/Azure/azhpc-images/blob/a3f92d283af6c9d11bf08eb0f8763ab67f7c8713/partners/rhel/common/install_utils.sh#L61
- name: Prevent installation of all kernel packages of a different version
  command: dnf versionlock add {{ item }}
  register: __hpc_versionlock_check
  changed_when: >-
    'Package already locked in equivalent form'
    not in __hpc_versionlock_check.stdout
  loop: "{{ __hpc_kernel_versionlock_rpms }}"

- name: Update all packages to bring system to the latest state
  when: hpc_update_all_packages
  package:
    name: "*"
    state: latest  # noqa package-latest

- name: Install NVidia driver
  # Note that currently the role supports only Microsoft Azure
  # When we add more cloud providers, we need to update this condition
  when: ansible_facts["system_vendor"] == "Microsoft Corporation"
  block:
    - name: Get list of dnf modules
      command: dnf module list
      register: __hpc_dnf_modules
      changed_when: false

    - name: Reset nvidia-driver module if it is enabled of different version
      when: __hpc_dnf_modules.stdout | regex_search('nvidia-driver (?!575-dkms).* \[e\]')
      command: dnf module reset nvidia-driver --assumeyes
      changed_when: true

    - name: Enable NVIDIA driver module
      vars:
        nvidia_enabled_pattern: >-
          {{ __hpc_nvidia_driver_module | regex_replace(':', ' ') + ' \[e\]' }}
      when: __hpc_dnf_modules.stdout is not search(nvidia_enabled_pattern)
      command: dnf module enable {{ __hpc_nvidia_driver_module }} --assumeyes
      notify: Reboot system
      changed_when: true

    - name: Install dkms
      package:
        name: "{{ __hpc_dkms_packages }}"
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_dkms_packages_install
      until: __hpc_dkms_packages_install is success

    - name: Install NVIDIA driver
      package:
        name: "{{ __hpc_nvidia_driver_packages }}"
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_nvidia_driver_packages_install
      until: __hpc_nvidia_driver_packages_install is success

    # This makes the role not idempotent.
    # We need to find a condition in which starting and enabling is not enough.
    - name: Restart dkms service to make it build nvidia drivers for all kernels
      service:
        name: dkms.service
        enabled: true
        state: restarted

- name: Install CUDA driver and enable nvidia-persistenced.service
  when:
    - ansible_facts["system_vendor"] == "Microsoft Corporation"
    - hpc_install_cuda_driver
  block:
    - name: Install CUDA driver
      package:
        name: "{{ __hpc_cuda_driver_packages }}"
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_cuda_driver_packages_install
      until: __hpc_cuda_driver_packages_install is success

    - name: Enable nvidia-persistenced.service
      service:
        name: nvidia-persistenced.service
        enabled: true

- name: Install CUDA Toolkit and lock version of its packages
  when:
    - hpc_install_cuda_toolkit
    - ansible_facts["system_vendor"] == "Microsoft Corporation"
  block:
    - name: Install CUDA Toolkit
      package:
        name: "{{ __hpc_cuda_toolkit_packages }}"
        state: present
        allow_downgrade: true
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_cuda_toolkit_packages_install
      until: __hpc_cuda_toolkit_packages_install is success

    - name: Prevent update of CUDA Toolkit packages
      command: dnf versionlock add {{ item }}
      register: __hpc_versionlock_check
      changed_when: >-
        'Package already locked in equivalent form'
        not in __hpc_versionlock_check.stdout
      loop: "{{ __hpc_kernel_versionlock_rpms }}"

- name: Install NVIDIA NCCL and lock version of its packages
  when: hpc_install_hpc_nvidia_nccl
  block:
    - name: Install NVIDIA NCCL
      package:
        name: "{{ __hpc_nvidia_nccl_packages }}"
        state: present
        allow_downgrade: true
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_nvidia_nccl_packages_install
      until: __hpc_nvidia_nccl_packages_install is success

    - name: Prevent update of NVIDIA NCCL packages
      command: dnf versionlock add {{ item }}
      register: __hpc_versionlock_check
      changed_when: >-
        'Package already locked in equivalent form'
        not in __hpc_versionlock_check.stdout
      loop: "{{ __hpc_nvidia_nccl_packages }}"

- name: Install NVIDIA Fabric Manager and enable service
  when: hpc_install_nvidia_fabric_manager
  block:
    - name: Install NVIDIA Fabric Manager
      package:
        name: "{{ __hpc_nvidia_fabric_manager_packages }}"
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_nvidia_fabric_manager_packages_install
      until: __hpc_nvidia_fabric_manager_packages_install is success

    - name: Ensure that Fabric Manager service is enabled
      service:
        name: nvidia-fabricmanager
        enabled: true

- name: Install RDMA packages
  when: hpc_install_rdma
  block:
    - name: Install RDMA packages
      package:
        name: "{{ __hpc_rdma_packages }}"
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_rdma_packages_install
      until: __hpc_rdma_packages_install is success

    # azhpc-images build scripts do the same thing here:
    # https://github.com/Azure/azhpc-images/blob/a3f92d283af6c9d11bf08eb0f8763ab67f7c8713/common/install_waagent.sh#L53
    - name: Enable RDMA in waagent configuration
      lineinfile:
        path: /etc/waagent.conf
        line: "OS.EnableRDMA=y"
        create: true
        backup: true
        mode: "0644"
      notify: Restart waagent

- name: Install common OpenMPI packages
  when: hpc_install_system_openmpi or hpc_build_openmpi_w_nvidia_gpu_support
  package:
    name: "{{ __hpc_openmpi_common_packages }}"
    state: present
    use: "{{ (__hpc_server_is_ostree | d(false)) |
      ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
  register: __hpc_openmpi_common_packages_install
  until: __hpc_openmpi_common_packages_install is success

- name: Install system OpenMPI
  when: hpc_install_system_openmpi
  package:
    name: "{{ __hpc_system_openmpi_packages }}"
    state: present
    use: "{{ (__hpc_server_is_ostree | d(false)) |
      ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
  register: __hpc_system_openmpi_packages_install
  until: __hpc_system_openmpi_packages_install is success

- name: Download, build, and configure OpenMPI and all required packages
  when: hpc_build_openmpi_w_nvidia_gpu_support
  block:
    - name: Install build dependencies
      package:
        name: >-
          {{ __hpc_pmix_build_dependencies
          + __hpc_gdrcopy_build_dependencies
          + __hpc_openmpi_build_dependencies }}
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

    # Must keep the original tarball name because it's hardcoded in hpcx
    - name: Set __hpc_hpcx_path fact
      vars:
        hpcx_tarball_basename: "{{ __hpc_hpcx_info.url | basename
          | regex_replace('\\.[^.]*$', '') }}"
      set_fact:
        __hpc_hpcx_path: "{{ __hpc_install_prefix }}/{{ hpcx_tarball_basename }}"

    - name: Set facts for building HPC-X and OpenMPI
      set_fact:
        __hpc_hpcx_rebuild_path: "{{ __hpc_hpcx_path }}/hpcx-rebuild"
        __hpc_hcoll_path: "{{ __hpc_hpcx_path }}/hcoll"
        __hpc_ucx_path: "{{ __hpc_hpcx_path }}/ucx/hpcx-rebuild"
        __hpc_ucc_path: "{{ __hpc_hpcx_path }}/ucc"
        __hpc_pmix_path: "{{ __hpc_install_prefix }}/{{ __hpc_pmix_info.name }}/{{ __hpc_pmix_info.version }}"
        __hpc_openmpi_path: "{{ __hpc_install_prefix }}/{{ __hpc_openmpi_info.name }}-{{ __hpc_openmpi_info.version }}"
        __hpc_cuda_path: /usr/local/cuda

    - name: Get stat of pmix path
      stat:
        path: "{{ __hpc_pmix_path }}/bin/pmixcc"
      register: __hpc_pmix_path_stat

    - name: Download and build PMIx
      when: not __hpc_pmix_path_stat.stat.exists
      block:
        - name: Download PMIx
          include_tasks: tasks/download_extract_package.yml
          vars:
            __hpc_pkg_info: "{{ __hpc_pmix_info }}"

        - name: Build PMIx
          command:
            cmd: "{{ item }}"
            chdir: "{{ __hpc_pkg_extracted.path }}"
          changed_when: true
          loop:
            - >-
              ./configure --prefix={{ __hpc_pmix_path }}
              --enable-pmix-binaries
              --disable-dependency-tracking
            - make -j {{ ansible_facts["processor_nproc"] }}
            - make install

    - name: Ensure PMIx modulefile directory exists
      file:
        path: "{{ __hpc_module_dir }}/pmix"
        state: directory
        owner: root
        group: root
        mode: '0755'

    # The openmpi-{{ __hpc_openmpi_info.version }} env module depends on this
    - name: Install PMIx modulefile
      template:
        src: pmix-ver.lua
        dest: "{{ __hpc_module_dir }}/pmix/pmix-{{ __hpc_pmix_info.version }}.lua"
        owner: root
        group: root
        mode: '0755'

    - name: Install GDRCopy packages
      vars:
        gdrcopy_version: "{{ __hpc_gdrcopy_info.version | split('-') | first }}"
      when: >-
        (ansible_facts.packages.gdrcopy is not defined
        or ansible_facts.packages.gdrcopy[0].version != gdrcopy_version )
        or (ansible_facts.packages["gdrcopy-kmod"] is not defined
        or ansible_facts.packages["gdrcopy-kmod"][0].version != gdrcopy_version)
        or (ansible_facts.packages["gdrcopy-devel"] is not defined
        or ansible_facts.packages["gdrcopy-devel"][0].version != gdrcopy_version)
      block:
        - name: Download GDRCopy
          include_tasks: tasks/download_extract_package.yml
          vars:
            __hpc_pkg_info: "{{ __hpc_gdrcopy_info }}"

        - name: Build GDRCopy RPM packages
          environment:
            CUDA: "{{ __hpc_cuda_path }}"
          command:
            cmd: packages/build-rpm-packages.sh
            chdir: "{{ __hpc_pkg_extracted.path }}"
          changed_when: true

        - name: Install GDRCopy packages from built RPMs
          package:
            name:
              - "{{ __hpc_pkg_extracted.path }}/gdrcopy-kmod-{{ __hpc_gdrcopy_info.version }}dkms.{{ __hpc_gdrcopy_info.distribution }}.noarch.rpm"
              - "{{ __hpc_pkg_extracted.path }}/gdrcopy-{{ __hpc_gdrcopy_info.version }}.{{ __hpc_gdrcopy_info.distribution }}.x86_64.rpm"
              - "{{ __hpc_pkg_extracted.path }}/gdrcopy-devel-{{ __hpc_gdrcopy_info.version }}.{{ __hpc_gdrcopy_info.distribution }}.noarch.rpm"
            disable_gpg_check: true
            state: present
            use: "{{ (__hpc_server_is_ostree | d(false)) |
              ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

        - name: Remove extracted tarball
          file:
            path: "{{ __hpc_pkg_extracted.path }}"
            state: absent
          changed_when: false

    - name: Get stat of hpcx-rebuild path
      stat:
        path: "{{ __hpc_hpcx_rebuild_path }}"
      register: __hpc_hpcx_rebuild_path_stat

    - name: Download and build HPC-X
      when: not __hpc_hpcx_rebuild_path_stat.stat.exists
      block:
        - name: Download HPC-X
          include_tasks: tasks/download_extract_package.yml
          vars:
            __hpc_pkg_info: "{{ __hpc_hpcx_info }}"

        # Packages are installed in /opt, not "/build-result", so pkgconfig
        # files used by openmpi's cmake configure scripts need to be updated.
        - name: Ensure that pkgconfig files use hpcx_home={{ __hpc_hpcx_path }}
          replace:
            path: "{{ item }}"
            regexp: "/build-result/"
            replace: "{{ __hpc_install_prefix }}/"
            backup: true
          loop:
            - "{{ __hpc_pkg_extracted.path }}/hcoll/lib/pkgconfig/hcoll.pc"
            - "{{ __hpc_pkg_extracted.path }}/ucc/lib/pkgconfig/ucc.pc"
            - "{{ __hpc_pkg_extracted.path }}/ucx/lib/pkgconfig/ucx.pc"

        # the shipped pkgconfig for hcoll only includes -lhcoll. However, this
        # library depends on -locoms, so we have to add that to prevent openmpi
        # from failing to link against libhcoll.
        - name: Update hcoll pkgconfig file to add -locoms parameter
          replace:
            path: "{{ __hpc_pkg_extracted.path }}/hcoll/lib/pkgconfig/hcoll.pc"
            regexp: "-lhcoll"
            replace: "-lhcoll -locoms"
            backup: true

        - name: Copy HPC-X files to {{ __hpc_hpcx_path }}
          copy:
            src: "{{ __hpc_pkg_extracted.path }}/"
            remote_src: true
            dest: "{{ __hpc_hpcx_path }}"
            mode: "0755"
            owner: root
            group: root

        - name: Rebuild HPC-X with PMIx
          environment:
            CUDA_HOME: "{{ __hpc_cuda_path }}"
          command: >-
            {{ __hpc_hpcx_path }}/utils/hpcx_rebuild.sh
            --with-hcoll
            --cuda
            --rebuild-ucx
            --ompi-extra-config
            '--with-pmix={{ __hpc_pmix_path }}
            --enable-orterun-prefix-by-default'
          changed_when: true

        - name: Copy ompi/tests to hpcx-rebuild in {{ __hpc_hpcx_path }}
          copy:
            src: "{{ __hpc_hpcx_path }}/ompi/tests/"
            remote_src: true
            dest: "{{ __hpc_hpcx_rebuild_path }}"
            mode: "0755"
            owner: root
            group: root

        - name: Remove extracted tarball
          file:
            path: "{{ __hpc_pkg_extracted.path }}"
            state: absent
          changed_when: false

    - name: Ensure MPI modulefile directory exists
      file:
        path: "{{ __hpc_module_dir }}/mpi"
        state: directory
        owner: root
        group: root
        mode: '0755'

    - name: Install NVidia HPCX OpemMPI modulefile
      template:
        src: hpcx-ver.lua
        dest: >-
          {{ __hpc_module_dir }}/mpi/hpcx-{{ __hpc_hpcx_info.version }}.lua
        owner: root
        group: root
        mode: '0755'

    - name: Install NVidia HPCX OpemMPI with PMIx {{ __hpc_pmix_info.version }}
      template:
        src: hpcx-ver-pmix-ver.lua
        dest: >-
          {{ __hpc_module_dir }}/mpi/hpcx-{{
          __hpc_hpcx_info.version }}-pmix-{{ __hpc_pmix_info.version }}.lua
        owner: root
        group: root
        mode: '0755'

    - name: Get stat of openmpi path
      stat:
        path: "{{ __hpc_openmpi_path }}"
      register: __hpc_openmpi_path_stat

    - name: Download and build OpenMPI
      when: not __hpc_openmpi_path_stat.stat.exists
      block:
        - name: Download {{ __hpc_openmpi_info.name }}
          include_tasks: tasks/download_extract_package.yml
          vars:
            __hpc_pkg_info: "{{ __hpc_openmpi_info }}"

        - name: Build {{ __hpc_openmpi_info.name }}
          command:
            cmd: "{{ item }}"
            chdir: "{{ __hpc_pkg_extracted.path }}"
          changed_when: true
          loop:
            - >-
              ./configure --prefix={{ __hpc_openmpi_path }}
              --with-ucx={{ __hpc_ucx_path }}
              --with-ucc={{ __hpc_ucc_path }}
              --with-hcoll={{ __hpc_hcoll_path }}
              --with-pmix={{ __hpc_pmix_path }}
              --enable-prte-prefix-by-default
              --with-platform=contrib/platform/mellanox/optimized
              --with-cuda={{ __hpc_cuda_path }}
            - make -j {{ ansible_facts["processor_nproc"] }}
            - make install

        - name: Remove extracted tarball
          file:
            path: "{{ __hpc_pkg_extracted.path }}"
            state: absent
          changed_when: false

    - name: Install OpenMPI modulefile
      template:
        src: openmpi-ver-cuda12-gpu.lua
        dest: "{{ __hpc_module_dir }}/mpi/openmpi-{{ __hpc_openmpi_info.version }}-cuda12-gpu.lua"
        owner: root
        group: root
        mode: '0755'

- name: Install Docker via moby-engine and moby-cli
  when: hpc_install_docker
  block:
    - name: Install Docker packages
      package:
        name: "{{ __hpc_docker_packages }}"
        state: present
        allow_downgrade: true
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_docker_packages_install
      until: __hpc_docker_packages_install is success

    - name: Enable and start Docker service
      service:
        name: docker
        enabled: true
        state: started

    - name: Prevent update of Docker packages
      command: dnf versionlock add {{ item }}
      register: __hpc_versionlock_check
      changed_when: >-
        'Package already locked in equivalent form'
        not in __hpc_versionlock_check.stdout
      loop: "{{ __hpc_docker_packages }}"

- name: Install and configure NVIDIA Container Toolkit
  when: hpc_install_nvidia_container_toolkit
  block:
    - name: Add NVIDIA Container Toolkit repository
      get_url:
        url: "{{ __hpc_nvidia_container_toolkit_repo.url }}"
        dest: /etc/yum.repos.d/nvidia-container-toolkit.repo
        mode: '0644'
        owner: root
        group: root

    - name: Install NVIDIA Container Toolkit
      package:
        name: "{{ __hpc_nvidia_container_toolkit_packages }}"
        state: present
        use: "{{ (__hpc_server_is_ostree | d(false)) |
          ternary('ansible.posix.rhel_rpm_ostree', omit) }}"
      register: __hpc_nvidia_container_toolkit_install
      until: __hpc_nvidia_container_toolkit_install is success

    - name: Prevent updates of NVIDIA Container Toolkit packages
      command: dnf versionlock add {{ item }}
      register: __hpc_versionlock_check
      changed_when: >-
        'Package already locked in equivalent form'
        not in __hpc_versionlock_check.stdout
      loop: "{{ __hpc_nvidia_container_toolkit_packages }}"

    - name: Check if NVIDIA runtime is configured in Docker daemon
      find:
        paths: /etc/docker
        patterns: daemon.json
        contains: nvidia
        read_whole_file: true
      register: docker_nvidia_runtime_check

    - name: Configure NVIDIA Container Toolkit for Docker runtime
      command: nvidia-ctk runtime configure --runtime=docker
      when: docker_nvidia_runtime_check.matched == 0
      changed_when: true
      notify: Restart docker service

    - name: Ensure containerd config directory exists
      file:
        path: /etc/containerd
        state: directory
        owner: root
        group: root
        mode: '0755'

    - name: Generate default containerd config
      shell:
        cmd: containerd config default > /etc/containerd/config.toml
        creates: /etc/containerd/config.toml

    - name: Enable SystemdCgroup in containerd config
      lineinfile:
        path: /etc/containerd/config.toml
        regexp: '^\s*SystemdCgroup\s*='
        line: '            SystemdCgroup = true'
        backup: true

    - name: Check if NVIDIA Container Toolkit drop-in file exists
      stat:
        path: /etc/containerd/conf.d/99-nvidia.toml
      register: nvidia_containerd_dropin

    - name: Check if containerd config has drop-in imports
      find:
        paths: /etc/containerd
        patterns: config.toml
        contains: 'imports = \["/etc/containerd/conf\.d/\*\.toml"\]'
      register: containerd_imports_check

    - name: Configure NVIDIA Container Toolkit for containerd runtime
      command: nvidia-ctk runtime configure --runtime=containerd --set-as-default
      when: not nvidia_containerd_dropin.stat.exists or containerd_imports_check.matched == 0
      changed_when: true
      notify: Restart containerd service

- name: Tune system for HPC
  when: hpc_tuning
  block:
    - name: Remove user memory limits to ensure applications aren't restricted
      template:
        src: 90-hpc-limits.conf
        dest: /etc/security/limits.d/
        owner: root
        group: root
        mode: "0644"

    - name: Add sysctl tuning configuration for HPC
      template:
        src: 90-hpc-sysctl.conf
        dest: /etc/sysctl.d/
        owner: root
        group: root
        mode: '0644'
      notify: Reload sysctl

    - name: Load sunrpc kernel module
      lineinfile:
        path: /etc/modules-load.d/sunrpc.conf
        line: sunrpc
        create: true
        owner: root
        group: root
        mode: '0644'
      notify: Restart systemd-modules-load

    - name: Check if sunrpc module is loaded
      command: lsmod
      register: __hpc_loaded_modules
      changed_when: false

    - name: Load sunrpc module if not loaded
      when: "'sunrpc' not in __hpc_loaded_modules.stdout"
      command: modprobe sunrpc
      changed_when: true

    - name: Copy NFS readahead udev rules for Azure infrastructure
      template:
        src: 90-nfs-readahead.rules
        dest: /etc/udev/rules.d/
        owner: root
        group: root
        mode: '0644'
      notify: Reload udev

- name: Create Azure HPC resource directories
  file:
    path: "{{ item }}"
    state: directory
    owner: root
    group: root
    mode: '0755'
  loop:
    - "{{ __hpc_azure_resource_dir }}"
    - "{{ __hpc_azure_resource_dir }}/bin"
    - "{{ __hpc_azure_tools_dir }}"
    - "{{ __hpc_azure_tests_dir }}"
    - "{{ __hpc_azure_runtime_dir }}"

- name: Install SKU Customisation scripts and services
  when: hpc_sku_customisation
  block:
    - name: Check if already installed
      stat:
        path: "{{ __hpc_azure_resource_dir }}/topology"
      register: __hpc_sku_topology_stat

    - name: Install files
      when: not __hpc_sku_topology_stat.stat.exists
      block:
        - name: Install Topology Definitions
          copy:
            src: sku/topology/
            dest: "{{ __hpc_azure_resource_dir }}/topology"
            owner: root
            group: root
            mode: '0755'

        - name: Install Graph Files
          copy:
            src: sku/customisations/
            dest: "{{ __hpc_azure_resource_dir }}/customisations"
            owner: root
            group: root
            mode: '0755'

        - name: Install setup script
          template:
            src: sku/setup_sku_customisations.sh
            dest: "{{ __hpc_azure_resource_dir }}/bin/setup_sku_customisations.sh"
            owner: root
            group: root
            mode: '0755'

        - name: Install removal script
          template:
            src: sku/remove_sku_customisations.sh
            dest: "{{ __hpc_azure_resource_dir }}/bin/remove_sku_customisations.sh"
            owner: root
            group: root
            mode: '0755'

        - name: Install systemd service file
          template:
            src: sku/sku_customisation.service
            dest: /etc/systemd/system/
            owner: root
            group: root
            mode: '0755'

        - name: Enable systemd service file
          service:
            name: sku_customisation.service
            enabled: true

        - name: Install tests
          template:
            src: sku/test-sku-setup.sh
            dest: "{{ __hpc_azure_tests_dir }}/"
            owner: root
            group: root
            mode: '0755'

- name: Remove build dependencies
  vars:
    __hpc_dependencies: >-
      {{ __hpc_gdrcopy_build_dependencies
      + __hpc_pmix_build_dependencies
      + __hpc_openmpi_build_dependencies }}
  package:
    name: "{{ __hpc_dependencies }}"
    state: absent
    use: "{{ (__hpc_server_is_ostree | d(false)) |
      ternary('ansible.posix.rhel_rpm_ostree', omit) }}"

- name: Install Moneo monitoring tool
  when: hpc_install_moneo
  block:
    - name: Check if Moneo is already installed
      stat:
        path: "{{ __hpc_azure_moneo_dir }}/moneo.py"
      register: __hpc_moneo_installed

    - name: Download and install Moneo
      when: not __hpc_moneo_installed.stat.exists
      block:
        - name: Ensure Moneo install directory exists
          file:
            path: "{{ __hpc_azure_tools_dir }}"
            state: directory
            mode: '0755'
            owner: root
            group: root

        - name: Download Moneo
          include_tasks: download_extract_package.yml
          vars:
            __hpc_pkg_info: "{{ __hpc_moneo_info }}"

        - name: Copy Moneo files to install directory
          copy:
            src: "{{ __hpc_pkg_extracted.path }}/"
            remote_src: true
            dest: "{{ __hpc_azure_moneo_dir }}"
            mode: '0755'
            owner: root
            group: root

        - name: Patch hardcoded paths in Moneo scripts
          replace:
            path: "{{ __hpc_azure_moneo_dir }}/linux_service/configure_service.sh"
            regexp: '/opt/azurehpc/tools/Moneo'
            replace: "{{ __hpc_azure_moneo_dir }}"

        - name: Configure Moneo service
          command: "{{ __hpc_azure_moneo_dir }}/linux_service/configure_service.sh"
          register: __hpc_moneo_configure
          changed_when: "'already configured' not in __hpc_moneo_configure.stdout"

        - name: Remove extracted temp directory
          file:
            path: "{{ __hpc_pkg_extracted.path }}"
            state: absent
          changed_when: false

    - name: Add Moneo alias to /etc/bashrc
      lineinfile:
        path: /etc/bashrc
        line: "alias moneo='python3 {{ __hpc_azure_moneo_dir }}/moneo.py'"
        state: present
        mode: '0644'

- name: Clean dnf cache
  command: dnf clean all
  changed_when: false